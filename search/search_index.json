{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to <code>ondil</code> - Online Distributional Learning","text":"<p><code>ondil</code> is a <code>Python</code> package for online distributional learning. We provide an online implementation of the well-known GAMLSS model using online coordinate descent (OCD).</p> <p>Note</p> <p><code>ondil</code> is currently in the first alpha phase. Please expect changes to happen frequently.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>This package provides an online estimation of models for distributional regression, respectively, models for conditional heteroskedastic data. The main contribution is an online/incremental implementation of the generalized additive models for location, shape and scale (GAMLSS, see Rigby &amp; Stasinopoulos, 2005) developed in Hirsch, Berrisch &amp; Ziel, 2024.</p> <p>Please have a look at the documentation or the example notebook.</p> <p>We're actively working on the package and welcome contributions from the community. Have a look at the Release Notes and the Issue Tracker.</p>"},{"location":"#distributional-regression","title":"Distributional Regression","text":"<p>The main idea of distributional regression (or regression beyond the mean, multiparameter regression) is that the response variable \\(Y\\) is distributed according to a specified distribution \\(\\mathcal{F}(\\theta)\\), where \\(\\theta\\) is the parameter vector for the distribution. In the Gaussian case, we have \\(\\theta = (\\theta_1, \\theta_2) = (\\mu, \\sigma)\\). We then specify an individual regression model for all parameters of the distribution of the form </p> \\[g_k(\\theta_k) = \\eta_k = X_k\\beta_k\\] <p>where \\(g_k(\\cdot)\\) is a link function, which ensures that the predicted distribution parameters are in a sensible range (we don't want, e.g. negative standard deviations), and \\(\\eta_k\\) is the predictor. For the Gaussian case, this would imply that we have two regression equations, one for the mean (location) and one for the standard deviation (scale) parameters. Distributions other than the normal distribution are possible, and we have already implemented them, e.g., Student's $ t$ distribution and Johnson's \\(S_U\\) distribution. If you are interested in another distribution, please open an Issue.</p> <p>This allows us to specify very flexible models that consider the conditional behaviour of the variable's volatility, skewness and tail behaviour. A simple example for electricity markets is wind forecasts, which are skewed depending on the production level - intuitively, there is a higher risk of having lower production if the production level is already high since it cannot go much higher than \"full load\" and if, the turbines might cut-off. Modelling these conditional probabilistic behaviours is the key strength of distributional regression models.</p>"},{"location":"#installation","title":"Installation","text":"<p><code>ondil</code> is available on the Python Package Index and can be installed via <code>pip</code>:</p> <pre><code>pip install ondil\n</code></pre>"},{"location":"#i-was-looking-for-rolch-but-i-found-ondil","title":"I was looking for <code>rolch</code> but I found <code>ondil</code>?","text":"<p><code>rolch</code> (Regularized Online Learning for Conditional Heteroskedasticity) was the original name of this package, but we decided to rename it to <code>ondil</code> (Online Distributional Learning) to better reflect its purpose and functionality, since conditional heteroskedasticity (=non constant variance) is just one of the many applications for distributional regression models that can be estimated with this package.</p>"},{"location":"#example","title":"Example","text":"<p>The following few lines give an introduction. We use the <code>diabetes</code> data set and model the response variable \\(Y\\) as Student-\\(t\\) distributed, where all distribution parameters (location, scale and tail) are modelled conditional on the explanatory variables in \\(X\\). We use LASSO to estimate the coefficients and the Bayesian information criterion to select the best model along a grid of regularization strengths.</p> <pre><code>import numpy as np\nfrom sklearn.datasets import load_diabetes\nfrom ondil.estimators import OnlineDistributionalRegression\nfrom ondil.distributions import StudentT\n\nX, y = load_diabetes(return_X_y=True)\n\n# Model coefficients\nequation = {\n    0: \"all\",  # Can also use \"intercept\" or np.ndarray of integers / booleans\n    1: \"all\",\n    2: \"all\",\n}\n\n# Create the estimator\nonline_gamlss_lasso = OnlineDistributionalRegression(\n    distribution=StudentT(),\n    method=\"lasso\",\n    equation=equation,\n    fit_intercept=True,\n    ic=\"bic\",\n)\n\n# Initial Fit\nonline_gamlss_lasso.fit(\n    X=X[:-11, :],\n    y=y[:-11],\n)\nprint(\"Coefficients for the first N-11 observations \\n\")\nprint(online_gamlss_lasso.beta)\n\n# Update call\nonline_gamlss_lasso.update(X=X[[-11], :], y=y[[-11]])\nprint(\"\\nCoefficients after update call \\n\")\nprint(online_gamlss_lasso.beta)\n\n# Prediction for the last 10 observations\nprediction = online_gamlss_lasso.predict_distribution_parameters(X=X[-10:, :])\n\nprint(\"\\n Predictions for the last 10 observations\")\n# Location, scale and shape (degrees of freedom)\nprint(prediction)\n</code></pre>"},{"location":"coordinate_descent/","title":"Online Coordinate Coordinate Descent","text":""},{"location":"coordinate_descent/#overview","title":"Overview","text":"<p>This submodule holds the functions related to the online coordinate descent (OCO).</p>"},{"location":"coordinate_descent/#api-reference","title":"API Reference","text":""},{"location":"coordinate_descent/#ondil.coordinate_descent.online_coordinate_descent","title":"ondil.coordinate_descent.online_coordinate_descent","text":"<pre><code>online_coordinate_descent(\n    x_gram: ndarray,\n    y_gram: ndarray,\n    beta: ndarray,\n    regularization: float,\n    is_regularized: bool,\n    alpha: float,\n    beta_lower_bound: ndarray,\n    beta_upper_bound: ndarray,\n    selection: Literal[\"cyclic\", \"random\"] = \"cyclic\",\n    tolerance: float = 0.0001,\n    max_iterations: int = 1000,\n) -&gt; Tuple[np.ndarray, int]\n</code></pre> <p>The parameter update cycle of the online coordinate descent.</p> <p>Parameters:</p> <ul> <li> <code>x_gram</code>               (<code>ndarray</code>)           \u2013            <p>X-Gramian \\(\\(X^TX\\)\\)</p> </li> <li> <code>y_gram</code>               (<code>ndarray</code>)           \u2013            <p>Y-Gramian \\(\\(X^TY\\)\\)</p> </li> <li> <code>beta</code>               (<code>ndarray</code>)           \u2013            <p>Current beta vector</p> </li> <li> <code>regularization</code>               (<code>float</code>)           \u2013            <p>Regularization parameter lambda</p> </li> <li> <code>is_regularized</code>               (<code>bool</code>)           \u2013            <p>Vector of bools indicating whether the coefficient is regularized</p> </li> <li> <code>beta_lower_bound</code>               (<code>ndarray</code>)           \u2013            <p>Lower bounds for beta</p> </li> <li> <code>beta_upper_bound</code>               (<code>ndarray</code>)           \u2013            <p>Upper bounds for beta</p> </li> <li> <code>selection</code>               (<code>Literal['cyclic', 'random']</code>, default:                   <code>'cyclic'</code> )           \u2013            <p>Apply cyclic or random coordinate descent. Defaults to \"cyclic\".</p> </li> <li> <code>tolerance</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for the beta update. Defaults to 1e-4.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum iterations. Defaults to 1000.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[ndarray, int]</code>           \u2013            <p>Tuple[np.ndarray, int]: Converged $$ \\beta $$</p> </li> </ul>"},{"location":"coordinate_descent/#ondil.coordinate_descent.online_coordinate_descent_path","title":"ondil.coordinate_descent.online_coordinate_descent_path","text":"<pre><code>online_coordinate_descent_path(\n    x_gram: ndarray,\n    y_gram: ndarray,\n    beta_path: ndarray,\n    lambda_path: ndarray,\n    is_regularized: ndarray,\n    alpha: float,\n    early_stop: int,\n    beta_lower_bound: ndarray,\n    beta_upper_bound: ndarray,\n    which_start_value: Literal[\n        \"previous_lambda\", \"previous_fit\", \"average\"\n    ] = \"previous_lambda\",\n    selection: Literal[\"cyclic\", \"random\"] = \"cyclic\",\n    tolerance: float = 0.0001,\n    max_iterations: int = 1000,\n) -&gt; Tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Run coordinate descent on a grid of regularization values.</p> <p>Parameters:</p> <ul> <li> <code>x_gram</code>               (<code>ndarray</code>)           \u2013            <p>X-Gramian \\(\\(X^TX\\)\\)</p> </li> <li> <code>y_gram</code>               (<code>ndarray</code>)           \u2013            <p>Y-Gramian \\(\\(X^TY\\)\\)</p> </li> <li> <code>beta_path</code>               (<code>ndarray</code>)           \u2013            <p>The current coefficent path</p> </li> <li> <code>lambda_path</code>               (<code>ndarray</code>)           \u2013            <p>The lambda grid</p> </li> <li> <code>is_regularized</code>               (<code>bool</code>)           \u2013            <p>Vector of bools indicating whether the coefficient is regularized</p> </li> <li> <code>early_stop</code>               (<code>int</code>)           \u2013            <p>Early stopping criterion. 0 implies no early stopping. Defaults to 0.</p> </li> <li> <code>beta_lower_bound</code>               (<code>ndarray</code>)           \u2013            <p>Lower bounds for beta</p> </li> <li> <code>beta_upper_bound</code>               (<code>ndarray</code>)           \u2013            <p>Upper bounds for beta</p> </li> <li> <code>which_start_value</code>               (<code>Literal['previous_lambda', 'previous_fit', 'average']</code>, default:                   <code>'previous_lambda'</code> )           \u2013            <p>Values to warm-start the coordinate descent. Defaults to \"previous_lambda\".</p> </li> <li> <code>selection</code>               (<code>Literal['cyclic', 'random']</code>, default:                   <code>'cyclic'</code> )           \u2013            <p>Apply cyclic or random coordinate descent. Defaults to \"cyclic\".</p> </li> <li> <code>tolerance</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for the beta update. Will be passed through to the parameter update. Defaults to 1e-4.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum iterations. Will be passed through to the parameter update. Defaults to 1000.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[ndarray, ndarray]</code>           \u2013            <p>Tuple[np.ndarray, np.ndarray]: Tuple with the updated coefficient path and the iteration count.</p> </li> </ul>"},{"location":"coordinate_descent/#ondil.coordinate_descent.soft_threshold","title":"ondil.coordinate_descent.soft_threshold","text":"<pre><code>soft_threshold(value: float, threshold: float)\n</code></pre> <p>The soft thresholding function.</p> <p>For value \\(x\\) and threshold \\(\\lambda\\), the soft thresholding function \\(S(x, \\lambda)\\) is defined as:</p> \\[S(x, \\lambda) = sign(x)(|x| - \\lambda)\\] <p>Parameters:</p> <ul> <li> <code>value</code>               (<code>float</code>)           \u2013            <p>The value</p> </li> <li> <code>threshold</code>               (<code>float</code>)           \u2013            <p>The threshold</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>float</code> )          \u2013            <p>The thresholded value</p> </li> </ul>"},{"location":"development/","title":"Development","text":"<p>First of all, we're happy that you want to take the time to contribute to this package and extending it. Thanks!</p> <p>This page is trying to give some guidance on how to write additional link functions or distributions and some of the ideas underlying the <code>LinkFunction</code>, the <code>Distribution</code> and the <code>ScipyMixin</code> classes.</p>"},{"location":"development/#writing-link-functions","title":"Writing Link Functions","text":"<p>Link functions map the predictors of the distribution parameter to the distribution parameter's support. Hence, we need to have the link function itself and its inverse. To calculate the score vector and the weights, we additionally need the first and second derivative of the link, as well as the derivative of the inverse.</p> <p>The <code>LinkFunction</code> abstract base class enforces, that <code>SomeNewLink()</code> implements:</p> <ul> <li>The link: <code>SomeNewLink().link()</code>.</li> <li>The inverse: <code>SomeNewLink().inverse()</code>.</li> <li>The first derivative of the link: <code>SomeNewLink().link_derivative()</code>.</li> <li>The second derivative of the link: <code>SomeNewLink().link_second_derivative()</code>.</li> <li>The derivative of the inverse: <code>SomeNewLink().inverse_derivative()</code>.</li> </ul> <p>Additionally, each link defines the <code>SomeNewLink().link_support</code> as <code>tuple</code> of <code>float</code> values. This is used to ensure at the initialization of a distribution that the link functions support is inside the support of the distribution parameter. We allow the link to shorten the possible outcome space of the parameter, but we don't allow to return impossible values. As an example, you can constrain the degrees of freedom \\(\\nu\\) of the \\(t\\)-distribution by taking a <code>LogShiftTwo()</code>, which ensures that \\(\\nu &gt; 2\\) (and therefore the variance exists), but you cannot choose the <code>Identity()</code>, since the degrees of freedom must be positive.</p> <p>There are two ways to implement the support of the link function:</p> <ul> <li> <p>As a class attribute, i.e. before the <code>__init__()</code>. This is usually done if parameters passed to the link function do not affect the support. We write something like</p> <pre><code>class SomeNewLink(LinkFunction):\n    link_support = (0, np.inf)\n\n    def __init__(self)\n        pass # or do something\n\n    def all_the_necessary_methods():\n        # implement links and derivative\n</code></pre> </li> <li> <p>As property to the class. This is usually done if the link functions parameter affect its support. Then we do this:</p> <pre><code>class SomeNewLink(LinkFunction):\n\n    def __init__(self, lower_support_bound):\n        self.lower_support_bound = lower_support_bound\n\n    @property\n    def link_support(self):\n        return (self.lower_support_bound, np.inf)\n\n    def all_the_necessary_methods():\n        # implement links and derivative\n</code></pre> </li> </ul> <p>To ensure that we model the support correctly - think about, e.g. \\(\\log(x)\\), we can use <code>np.nextafter(a, b)</code>, which gives the next possible float after <code>a</code> in the direction of <code>b</code>. Hence, <code>np.nextafter(0, 1)</code> will return a very small positive float, which describes the support of \\(\\log(x)\\) well.</p>"},{"location":"development/#writing-distributions","title":"Writing Distributions","text":"<p>Distributions are an integral part of the package. All distributions inherit from <code>Distribution</code>, an abstract base class. This class enforces that each distribution has certain methods and members and also provides some default methods.</p> <p>The <code>Distribution</code> class is a very general class. However, some distributions have the corresponding implementation in <code>scipy.stats</code>. To use this, the <code>ScipyMixin</code> class provides the already existing implementations of <code>cdf</code>, <code>pdf</code> etc. This makes implementing a new distribution very easy.</p>"},{"location":"development/#scipy-distributions","title":"Scipy Distributions","text":"<p>Let's consider the <code>Normal</code> distribution as an example. The <code>Normal</code> distribution is implemented in <code>scipy.stats</code> as <code>scipy.stats.norm</code>. Hence, we can use the <code>ScipyMixin</code> together with <code>Distribution</code> to implement the <code>Normal</code> distribution. The <code>Normal</code> distribution is implemented as follows:</p> <pre><code>class Normal(Distribution, ScipyMixin):\n    \"\"\"Corresponds to GAMLSS NO() and scipy.stats.norm()\"\"\"\n\n corresponding_gamlss: str = \"NO\"\n\n parameter_names = {0: \"mu\", 1: \"sigma\"}\n parameter_support = {0: (-np.inf, np.inf), 1: (np.nextafter(0, 1), np.inf)}\n distribution_support = (-np.inf, np.inf)\n\n    # Scipy equivalent and parameter mapping ondil -&gt; scipy\n scipy_dist = st.norm\n scipy_names = {\"mu\": \"loc\", \"sigma\": \"scale\"}\n</code></pre> <p>First, we assign a name and declare the inheritance. Then, we define some class properties like <code>corresponding_gamlss</code>, <code>parameter_names</code>, <code>parameter_support</code> etc. These are enforced through <code>@property</code> decorators in <code>Distribution</code> and <code>ScipyMixin</code>.</p> <p>Note that <code>parameter_names</code> relates the variable names used throughout the methods to the columns of the parameter array <code>theta</code>. In turn, <code>scipy_names</code> relates these variable names to the argument names of the <code>scipy.stats</code> distribution. This is necessary to map the parameters correctly.</p> <p>If possible, we follow the naming conventions of the <code>gamlss</code> package:</p> <pre><code>0 : Location\n1 : Scale (close to standard deviation)\n2 : Skewness\n3 : Tail behaviour\n</code></pre> <p>Then, we add an initialization method:</p> <pre><code>def __init__(\n self,\n loc_link: LinkFunction = Identity(),\n scale_link: LinkFunction = Log(),\n) -&gt; None:\n    super().__init__(\n        links={\n            0: loc_link,\n            1: scale_link,\n }\n )\n</code></pre> <p>This must provide the links for the parameters to the initializer of the base class.</p> <p>Furthermore, we need to define a method <code>initial_values</code>, which provides the initial values for the distribution parameters:</p> <pre><code>def initial_values(\n self, y: np.ndarray, param: int = 0, axis: Optional[int | None] = None\n) -&gt; np.ndarray:\n    if param == 0:\n        return np.repeat(np.mean(y, axis=axis), y.shape[0])\n    if param == 1:\n        return np.repeat(np.std(y, axis=axis), y.shape[0])\n</code></pre> <p>Lastly, we need to implement the log-likelihood function's first, second and cross-derivatives: <code>dl1_dp1</code>, <code>dl2_dp2</code>, and <code>dl2_dpp</code> respectively.</p> <p>Thats it! We implemented the <code>Normal</code> distribution.</p>"},{"location":"development/#special-cases","title":"Special Cases","text":"<p>Some distributions may be special. For example, our implementation of the Gamma distribution <code>Gamma</code> uses a different parameterization than <code>scipy.stats.gamma</code>. In consequence, we cannot use <code>theta_to_scipy_params()</code> from <code>ScipyMixin</code> to map the parameters to <code>scipy</code>. In this case, we must implement the <code>theta_to_scipy_params()</code> method ourselves. This is done in the <code>Gamma</code> class:</p> <pre><code>def theta_to_scipy_params(self, theta: np.ndarray) -&gt; dict:\n    \"\"\"Map GAMLSS Parameters to scipy parameters.\n\n Args:\n theta (np.ndarray): parameters\n\n Returns:\n dict: Dict of (a, loc, scale) for scipy.stats.gamma(a, loc, scale)\n \"\"\"\n mu = theta[:, 0]\n sigma = theta[:, 1]\n beta = 1 / (sigma**2 * mu)\n params = {\"a\": 1 / sigma**2, \"loc\": 0, \"scale\": 1 / beta}\n    return params\n</code></pre> <p>By providing a method called <code>theta_to_scipy_params</code> in the distribution class, we overwrite the default implementation from <code>ScipyMixin</code> and can map the parameters correctly.</p>"},{"location":"development/#summary","title":"Summary","text":"<p>Implementing your own distribution is fairly straightforward, especially if there is already a <code>scipy.stats</code> implementation. A good starting point is to look at the <code>Normal</code> function.</p> <p>If things need to be adjusted, then <code>Gamma</code> demonstrates how to do this.</p> <p>If you want to implement a distribution that is not available in <code>scipy.stats</code>, then you need to implement some methods, including <code>cdf</code>, <code>pdf</code>, and <code>rvs</code> yourself. In that case, your new class will inherit only from <code>Distribution</code>. You can inspect <code>ScipyMixin</code> to see which other methods need to be implemented.</p>"},{"location":"distributions/","title":"Distributions","text":"<p>This serves as reference for all distribution objects that we implement in the <code>ondil</code> package. </p> <p>Note</p> <p>This page is somewhat under construction, since <code>MkDocs</code> does not support docstring inheritance at the moment.</p> <p>All distributions are based on <code>scipy.stats</code> distributions. We implement the probability density function (PDF), the cumulative density function (CDF), the percentage point or quantile function (PPF) and the random variates (RVS) accordingly as pass-through. The link functions are implemented in the same way as in GAMLSS (Rigby &amp; Stasinopoulos, 2005). The link functions and their derivatives derive from the <code>LinkFunction</code> base class.</p>"},{"location":"distributions/#base-classes","title":"Base Classes","text":"Base Distribution Description <code>Distribution</code> Base class for all distributions. <code>ScipyMixin</code> Base class for all distributions that are based on <code>scipy</code>."},{"location":"distributions/#list-of-distributions","title":"List of Distributions","text":"Distribution Description <code>scipy</code> Base <code>Normal</code> Gaussian (mean and standard deviation) <code>scipy.stats.norm</code> <code>NormalMeanVariance</code> Gaussian (mean and variance) <code>scipy.stats.norm</code> <code>StudentT</code> Student's \\(t\\) distribution <code>scipy.stats.t</code> <code>SkewT</code> Skewed Student's \\(t\\) distribution - <code>SkewTMeanStd</code> Skewed Student's \\(t\\) distribution (mean and standard deviation) - <code>JSU</code> Johnson's SU distribution <code>scipy.stats.johnsonsu</code> <code>Gamma</code> Gamma distribution <code>scipy.stats.gamma</code> <code>LogNormal</code> Log-normal distribution <code>scipy.stats.lognorm</code> <code>LogNormalMedian</code> Log-normal distribution (median) - <code>Logistic</code> Logistic distribution <code>scipy.stats.logistic</code> <code>Exponential</code> Exponential distribution <code>scipy.stats.expon</code> <code>Beta</code> Beta distribution <code>scipy.stats.beta</code> <code>Gumbel</code> Gumbel distribution <code>scipy.stats.gumbel_r</code> <code>InverseGaussian</code> Inverse Gaussian distribution <code>scipy.stats.invgauss</code> <code>BetaInflated</code> Beta Inflated distribution - <code>ReverseGumbel</code> Reverse Gumbel distribution <code>scipy.stats.gumbel_r</code> <code>InverseGamma</code> Inverse Gamma distribution <code>scipy.stats.invgamma</code> <code>BetaInflatedZero</code> Zero Inflated Beta distribution - <code>ZeroAdjustedGamma</code> Zero Adjusted Gamma distribution - <code>PowerExponential</code> Power Exponential distribution - <code>Weibull</code> Weibull distribution <code>scipy.stats.weibull_min</code> Distribution Description Scale Matrix Parameterization Formula <code>MultivariateNormalInverseCholesky</code> Multivariate normal (inverse Cholesky) Inverse Cholesky factorization $\\Sigma = (L L^{\\top})^{-1}$, where $L$ is lower triangular <code>MultivariateNormalInverseModifiedCholesky</code> Multivariate normal (inverse modified Cholesky) Inverse modified Cholesky factorization $\\Sigma = (T D T^{\\top})^{-1}$, $T$ unit lower triangular, $D$ diagonal <code>MultivariateNormalInverseLowRank</code> Multivariate normal (inverse low-rank) Inverse low-rank factorization $\\Sigma = (U U^{\\top} + D)^{-1}$, $U$ low-rank, $D$ diagonal <code>MultivariateStudentTInverseCholesky</code> Multivariate Student's $t$ (inverse Cholesky) Inverse Cholesky factorization $\\Sigma = (L L^{\\top})^{-1}$, where $L$ is lower triangular <code>MultivariateStudentTInverseModifiedCholesky</code> Multivariate Student's $t$ (inverse modified Cholesky) Inverse modified Cholesky factorization $\\Sigma = (T D T^{\\top})^{-1}$, $T$ unit lower triangular, $D$ diagonal <code>MultivariateStudentTInverseLowRank</code> Multivariate Student's $t$ (inverse low-rank) Inverse low-rank factorization $\\Sigma = (U U^{\\top} + D)^{-1}$, $U$ low-rank, $D$ diagonal"},{"location":"distributions/#api-reference","title":"API Reference","text":""},{"location":"distributions/#ondil.distributions.Normal","title":"ondil.distributions.Normal","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Normal distribution with mean and standard deviation parameterization.</p> <p>The probability density function of the distribution is defined as: $$     f(y | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right). $$ respectively $$     f(y | \\theta_0, \\theta_1) = \\frac{1}{\\sqrt{2\\pi\\theta_1^2}} \\exp\\left(-\\frac{(y - \\theta_0)^2}{2\\theta_1^2}\\right). $$ where \\(y\\) is the observed data, \\(\\mu = \\theta_0\\) is the location parameter and \\(\\sigma = \\theta_1\\) is the scale parameter.</p> <p>This distribution corresponds to the NO() distribution in GAMLSS.</p>"},{"location":"distributions/#ondil.distributions.Normal.__init__","title":"__init__","text":"<pre><code>__init__(\n    loc_link: LinkFunction = Identity(),\n    scale_link: LinkFunction = Log(),\n) -&gt; None\n</code></pre> <p>Initialize the Normal.</p> <p>Parameters:</p> <ul> <li> <code>loc_link</code>               (<code>LinkFunction</code>, default:                   <code>Identity()</code> )           \u2013            <p>Location link. Defaults to Identity().</p> </li> <li> <code>scale_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>Scale link. Defaults to Log().</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.NormalMeanVariance","title":"ondil.distributions.NormalMeanVariance","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Normal distribution with mean and variance parameterization.</p> <p>The probability density function of the distribution is defined as: $$     f(y | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right). $$ respectively $$     f(y | \\theta_0, \\theta_1) = \\frac{1}{\\sqrt{2\\pi\\theta_1}} \\exp\\left(-\\frac{(y - \\theta_0)^2}{2\\theta_1}\\right). $$ where \\(y\\) is the observed data, \\(\\mu = \\theta_0\\) is the location parameter and \\(\\sigma^2 = \\theta_1\\) is the scale parameter.</p>"},{"location":"distributions/#ondil.distributions.NormalMeanVariance.__init__","title":"__init__","text":"<pre><code>__init__(\n    loc_link: LinkFunction = Identity(),\n    scale_link: LinkFunction = Log(),\n) -&gt; None\n</code></pre> <p>Initialize the NormalMeanVariance.</p> <p>Parameters:</p> <ul> <li> <code>loc_link</code>               (<code>LinkFunction</code>, default:                   <code>Identity()</code> )           \u2013            <p>Location link. Defaults to Identity().</p> </li> <li> <code>scale_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>Scale link. Defaults to Log().</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.NormalMeanVariance.theta_to_scipy_params","title":"theta_to_scipy_params","text":"<pre><code>theta_to_scipy_params(theta: ndarray) -&gt; dict\n</code></pre> <p>Map GAMLSS Parameters to scipy parameters.</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>parameters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>Dict of (loc, scale) for scipy.stats.norm(loc, scale)</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.LogNormal","title":"ondil.distributions.LogNormal","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Log-Normal distribution with mean and standard deviation parameterization in the log-space.</p> <p>The probability density function of the distribution is defined as: $$ f(y | \\mu, \\sigma) = \\frac{1}{y\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\log y - \\mu)^2}{2\\sigma^2}\\right). $$  respectively $$ f(y | \\theta_0, \\theta_1) = \\frac{1}{y\\theta_1\\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\log y - \\theta_0)^2}{2\\theta_1^2}\\right). $$ where \\(y\\) is the observed data, \\(\\mu = \\theta_0\\) is the location parameter and \\(\\sigma = \\theta_1\\) is the scale parameter.</p> <p>Note</p> <p>Note that re-parameterization used to move from scipy.stats to GAMLSS is: $$     \\mu = \\exp(\\theta_0) $$ and can therefore be numerically unstable for large values of \\(\\theta_0\\). We have re-implemented the PDF, CDF, PPF according to avoid this issue, however the rvs method still uses the scipy.stats implementation which is not numerically stable for large values of \\(\\theta_0\\).</p>"},{"location":"distributions/#ondil.distributions.LogNormal.pdf","title":"pdf","text":"<pre><code>pdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Probability density function of the Log-Normal distribution.</p>"},{"location":"distributions/#ondil.distributions.LogNormal.cdf","title":"cdf","text":"<pre><code>cdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Cumulative distribution function of the Log-Normal distribution.</p>"},{"location":"distributions/#ondil.distributions.LogNormal.ppf","title":"ppf","text":"<pre><code>ppf(p: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Percent-point function (quantile function) of the Log-Normal distribution.</p>"},{"location":"distributions/#ondil.distributions.LogNormal.logpdf","title":"logpdf","text":"<pre><code>logpdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Logarithm of the probability density function of the Log-Normal distribution.</p>"},{"location":"distributions/#ondil.distributions.LogNormalMedian","title":"ondil.distributions.LogNormalMedian","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Log-Normal distribution with median and standard deviation parameterization in the log-space.</p> <p>The probability density function of the distribution is defined as: $$ f(y | \\mu, \\sigma) = \\frac{1}{y\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log y - \\log \\mu)^2}{2\\sigma^2}\\right). $$ respectively $$ f(y | \\theta_0, \\theta_1) = \\frac{1}{y\\theta_1\\sqrt{2\\pi}}\\exp\\left(-\\frac{(\\log y - \\log \\theta_0)^2}{2\\theta_1^2}\\right). $$ where \\(y\\) is the observed data, \\(\\mu = \\theta_0\\) is the median parameter and \\(\\sigma = \\theta_1\\) is the scale parameter.</p>"},{"location":"distributions/#ondil.distributions.StudentT","title":"ondil.distributions.StudentT","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>Corresponds to GAMLSS TF() and scipy.stats.t()</p>"},{"location":"distributions/#ondil.distributions.SkewT","title":"ondil.distributions.SkewT","text":"<p>               Bases: <code>Distribution</code></p>"},{"location":"distributions/#ondil.distributions.SkewTMeanStd","title":"ondil.distributions.SkewTMeanStd","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p>"},{"location":"distributions/#ondil.distributions.JSU","title":"ondil.distributions.JSU","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>Corresponds to GAMLSS JSUo() and scipy.stats.johnsonsu()</p> <p>Distribution parameters: 0 : Location 1 : Scale (close to standard deviation) 2 : Skewness 3 : Tail behaviour</p>"},{"location":"distributions/#ondil.distributions.Gamma","title":"ondil.distributions.Gamma","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Gamma Distribution for GAMLSS.</p> <p>The distribution function is defined as in GAMLSS as: $$ f(y|\\mu,\\sigma)=\\frac{y^{(1/\\sigma^2-1)}\\exp[-y/(\\sigma^2 \\mu)]}{(\\sigma^2 \\mu)^{(1/\\sigma^2)} \\Gamma(1/\\sigma^2)} $$</p> <p>with the location and shape parameters \\(\\mu, \\sigma &gt; 0\\).</p> <p>Note</p> <p>The function is parameterized as GAMLSS' GA() distribution.</p> <p>This parameterization is different to the <code>scipy.stats.gamma(alpha, loc, scale)</code> parameterization.</p> <p>We can use <code>Gamma().theta_to_scipy_params(theta)</code> to map the distribution parameters to scipy.</p> <p>The <code>scipy.stats.gamma()</code> distribution is defined as: $$ f(x, \\alpha, \\beta) = \\frac{\\beta^\\alpha x^{\\alpha - 1} \\exp[-\\beta x]}{\\Gamma(\\alpha)} $$ with the paramters \\(\\alpha, \\beta &gt;0\\). The parameters can be mapped as follows: $$ \\alpha = 1/\\sigma^2 \\Leftrightarrow \\sigma = \\sqrt{1 / \\alpha} $$ and $$ \\beta = 1/(\\sigma^2\\mu). $$</p> <p>Parameters:</p> <ul> <li> <code>loc_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>The link function for \\(\\mu\\). Defaults to Log().</p> </li> <li> <code>scale_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>The link function for \\(\\sigma\\). Defaults to Log().</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.Gamma.theta_to_scipy_params","title":"theta_to_scipy_params","text":"<pre><code>theta_to_scipy_params(theta: ndarray) -&gt; dict\n</code></pre> <p>Map GAMLSS Parameters to scipy parameters.</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>parameters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>Dict of (a, loc, scale) for scipy.stats.gamma(a, loc, scale)</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.InverseGamma","title":"ondil.distributions.InverseGamma","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Inverse Gamma distribution as parameterized in GAMLSS:</p> The distribution has two parameters <ul> <li>mu: mean-related parameter</li> <li>sigma: dispersion parameter</li> </ul> Reparameterization <p>\u03b1 = 1 / sigma\u00b2 scale = mu * (1 + sigma\u00b2) / sigma\u00b2</p> <p>This distribution corresponds to IGAMMA() in GAMLSS.</p>"},{"location":"distributions/#ondil.distributions.ZeroAdjustedGamma","title":"ondil.distributions.ZeroAdjustedGamma","text":"<p>               Bases: <code>Distribution</code></p> <p>The Zero Adjusted Gamma Distribution for GAMLSS.</p> <p>The zero adjusted gamma distribution is a mixture of a discrete value 0 with probability \\nu, and a gamma GA(\\mu; \\sigma) distribution on the positive real line (0, \\infty) with probability (1 - \\nu).</p> \\[ f_Y(y \\mid \\mu, \\sigma, \\nu) =  \\begin{cases}     \\nu &amp; \\text{if } y = 0 \\     (1 - \\nu) f_W(y \\mid \\mu, \\sigma) &amp; \\text{if } y &gt; 0 \\end{cases}  \\] <p>where \\(y\\) is the observed data, \\(\\mu &gt; 0\\) is the location parameter, \\(\\sigma &gt; 0\\) is the scale parameter, and $\\nu \\in [0, \\infty) $  is the inflation parameter.</p>"},{"location":"distributions/#ondil.distributions.Logistic","title":"ondil.distributions.Logistic","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Logistic distribution with location and scale parameterization.</p> <p>The probability density function is: $$ f(y | \\mu, \\sigma) = \\frac{\\exp\\left(-\\frac{y - \\mu}{\\sigma}\\right)}{\\sigma \\left(1 + \\exp\\left(-\\frac{y - \\mu}{\\sigma}\\right)\\right)^2} $$</p> <p>This distribution corresponds to the LO() distribution in GAMLSS.</p>"},{"location":"distributions/#ondil.distributions.Exponential","title":"ondil.distributions.Exponential","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Exponential distribution parameterized by the mean (mu).</p> <p>PDF: f(y | mu) = (1 / mu) * exp(-y / mu), for y &gt; 0, mu &gt; 0</p> <p>This corresponds to EXP() in GAMLSS where: - mu &gt; 0 - y &gt; 0</p>"},{"location":"distributions/#ondil.distributions.PowerExponential","title":"ondil.distributions.PowerExponential","text":"<p>               Bases: <code>Distribution</code></p> <p>Power Exponential distribution (GAMLSS: PE).</p> <p>The PDF is defined as:</p> <p>\\(f(y \\mid \\mu, \\sigma, \\nu) = \\left(\\frac{\\nu}{2 \\sigma c(\\nu)}\\right) \\exp\\left(-0.5 \\left|\\frac{y - \\mu}{\\sigma c(\\nu)}\\right|^{\\nu}\\right)\\)</p> <p>where \\(c(\\nu) = \\sqrt{2} \\left(\\frac{\\Gamma(1/\\nu)}{\\Gamma(3/\\nu)}\\right)^{1/2}\\) and \\(\\Gamma(\\cdot)\\) is the gamma function.</p> <p>The parameters are: \\(\\mu\\) (location), \\(\\sigma\\) (scale), and \\(\\nu\\) (shape).</p>"},{"location":"distributions/#ondil.distributions.InverseGaussian","title":"ondil.distributions.InverseGaussian","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>Inverse Gaussian (Wald) distribution for GAMLSS.</p> <p>This distribution is characterized by two parameters: - \\(\\mu\\): the mean of the distribution. - \\(\\sigma\\): the scale parameter, which is related to the variance.</p> <p>The probability density function (PDF) is given by: $$     f(y; \\mu, \\sigma) = \\sqrt{\\frac{\\sigma}{2\\pi y^3}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2 y}\\right) $$ where \\(y &gt; 0\\), \\(\\mu &gt; 0\\), and \\(\\sigma &gt; 0\\).</p> <p>Note that the Inverse Gaussian distribution in <code>scipy.stats</code> is parameterized differently:</p> <ul> <li><code>mu</code> is the mean of the distribution.</li> <li><code>scale</code> is the scale parameter</li> </ul> <p>and the PDF is given by: $$     f(y; \\mu, \\lambda) = \\sqrt{\\frac{\\lambda}{2\\pi y^3}} \\exp\\left(-\\frac{\\lambda (y - \\mu)^2}{2\\mu^2 y}\\right) $$ where \\(y &gt; 0\\), \\(\\mu &gt; 0\\), and \\(\\lambda &gt; 0\\).</p> <p>The relationship between the parameters is:</p> <ul> <li><code>mu</code> in <code>scipy.stats</code> corresponds to \\(\\mu \\sigma^2\\) in this implementation,</li> <li><code>scale</code> in <code>scipy.stats</code> corresponds to \\(1 / \\sigma^2\\) in this implementation.</li> <li>The <code>loc</code> parameter in <code>scipy.stats</code> is always 0.</li> </ul>"},{"location":"distributions/#ondil.distributions.Beta","title":"ondil.distributions.Beta","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Beta Distribution for GAMLSS.</p> <p>The distribution function is defined as in GAMLSS as: $$ f(y|\\mu,\\sigma)=\\frac{\\Gamma(\\frac{1 - \\sigma^2}{\\sigma^2})}     {     \\Gamma(\\frac{\\mu (1 - \\sigma^2)}{\\sigma^2})     \\Gamma(\\frac{(1 - \\mu) (1 - \\sigma^2)}{\\sigma^2})}     y^{\\frac{\\mu (1 - \\sigma^2)}{\\sigma^2} - 1}     (1-y)^{\\frac{(1 - \\mu) (1 - \\sigma^2)}{\\sigma^2} - 1} $$</p> <p>with the location and shape parameters \\(\\mu, \\sigma &gt; 0\\).</p> <p>Note</p> <p>The function is parameterized as GAMLSS' BE() distribution.</p> <p>This parameterization is different to the <code>scipy.stats.beta(alpha, beta, loc, scale)</code> parameterization.</p> <p>We can use <code>Beta().gamlss_to_scipy(mu, sigma)</code> to map the distribution parameters to scipy.</p> <p>The <code>scipy.stats.beta()</code> distribution is defined as: $$ f(x, \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta) x^{\\alpha - 1} {(1 - x)}^{\\beta - 1}}{\\Gamma(\\alpha) \\Gamma(\\beta)} $$</p> <p>with the paramters \\(\\alpha, \\beta &gt;0\\). The parameters can be mapped as follows: $$ \\alpha = \\mu (1 - \\sigma^2) / \\sigma^2 \\Leftrightarrow \\mu = \\alpha / (\\alpha + \\beta) $$ and $$ \\beta = (1 - \\mu) (1 - \\sigma^2)/ \\sigma^2 \\Leftrightarrow \\sigma = \\sqrt{((\\alpha + \\beta + 1) )} $$</p> <p>Parameters:</p> <ul> <li> <code>loc_link</code>               (<code>LinkFunction</code>, default:                   <code>Logit()</code> )           \u2013            <p>The link function for \\(\\mu\\). Defaults to  LOGIT</p> </li> <li> <code>scale_link</code>               (<code>LinkFunction</code>, default:                   <code>Logit()</code> )           \u2013            <p>The link function for \\(\\sigma\\). Defaults to LOGIT</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.Beta.theta_to_scipy_params","title":"theta_to_scipy_params","text":"<pre><code>theta_to_scipy_params(theta: ndarray) -&gt; dict\n</code></pre> <p>Map GAMLSS Parameters to scipy parameters.</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>parameters</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>dict</code> )          \u2013            <p>Dict of (a, b, loc, scale) for scipy.stats.beta(a, b, loc, scale)</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.BetaInflated","title":"ondil.distributions.BetaInflated","text":"<p>               Bases: <code>Distribution</code></p> <p>The Beta Inflated Distribution for GAMLSS.</p> <p>The distribution function is defined as in GAMLSS as: $$ f_Y(y \\mid \\mu, \\sigma, \\nu, \\tau) =  \\begin{cases} p_0 &amp; \\text{if } y = 0 \\ (1 - p_0 - p_1) \\dfrac{1}{B(\\alpha, \\beta)} y^{\\alpha - 1}(1 - y)^{\\beta - 1} &amp; \\text{if } 0 &lt; y &lt; 1 \\ p_1 &amp; \\text{if } y = 1 \\end{cases} $$</p> <p>where \\(\\alpha = \\mu (1 - \\sigma^2) / \\sigma^2\\), \\beta = (1 - \\mu) (1 - \\sigma^2)/ \\sigma^2;  p_0 = \\nu (1 + \\nu + \\tau)^{-1} and p_1 =  \\tau (1 + \\nu + \\tau)^{-1}$, </p> <p>and \\(\\mu, \\sigma \\in (0,1)\\) and $\\nu, \\tau &gt; 0 $</p> <p>The parameter tuple \\(\\theta\\) in Python is defined as:</p> <p>\\(\\theta = (\\theta_0, \\theta_1, \\theta_2, \\theta_3) = (\\mu, \\sigma, \\nu, \\tau)\\)  where \\(\\mu = \\theta_0\\) is the location parameter, \\(\\sigma = \\theta_1\\) is the scale parameter  and \\(\\nu, \\tau = \\theta_2, \\theta_3\\) are shape parameters which together define the inflation at 0 and 1</p> <p>This distribution corresponds to the BEINF() distribution in GAMLSS.</p>"},{"location":"distributions/#ondil.distributions.BetaInflatedZero","title":"ondil.distributions.BetaInflatedZero","text":"<p>               Bases: <code>Distribution</code></p> <p>The Zero Inflated Beta Distribution for GAMLSS.</p> <p>f_Y(y \\mid \\mu, \\sigma, \\nu) =  \\begin{cases} p_0 &amp;       ext{if } y = 0 \\ (1 - p_0) f_W(y \\mid \\mu, \\sigma) &amp; \\text{if } 0 &lt; y &lt; 1 \\end{cases}</p> <p>where  \\(p_0 = \\nu (1 + \\nu)^{-1}\\)</p> <p>and \\(\\mu, \\sigma \\in (0,1)\\) and $\\nu &gt; 0 $</p>"},{"location":"distributions/#ondil.distributions.Gumbel","title":"ondil.distributions.Gumbel","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Gumbel distribution.</p> <p>The probability density function is given by: $$     f(y|\\mu, \\sigma) = (1/\\sigma) * \\exp(-(z + \\exp(-z))) $$ where \\(z = (y - \\mu)/\\sigma\\) and has the following parameters:</p> <ul> <li>\\(\\mu\\): location</li> <li>\\(\\sigma\\): scale (&gt;0)</li> </ul>"},{"location":"distributions/#ondil.distributions.ReverseGumbel","title":"ondil.distributions.ReverseGumbel","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Reverse Gumbel (Type I minimum extreme value) distribution with location (mu) and scale (sigma) parameters.</p> <p>The probability density function is defined as: $$ f(y | \\mu, \\sigma) = \\frac{1}{\\sigma} \\exp\\left( \\frac{y - \\mu}{\\sigma} - \\exp\\left( \\frac{y - \\mu}{\\sigma} \\right) \\right) $$</p> <p>This distribution corresponds to the RG() distribution in GAMLSS.</p> Notes <ul> <li>Mean = mu - digamma(1) * sigma \u2248 mu - 0.5772157 * sigma</li> <li>Variance = (pi^2 * sigma^2) / 6 \u2248 1.64493 * sigma^2</li> </ul>"},{"location":"distributions/#ondil.distributions.Weibull","title":"ondil.distributions.Weibull","text":"<p>               Bases: <code>ScipyMixin</code>, <code>Distribution</code></p> <p>The Weibull distribution parameterized by scale (mu) and shape (sigma).</p> <p>The probability density function is defined as: $$ f(y | \\mu, \\sigma) = \\frac{\\sigma}{\\mu} \\left(\\frac{y}{\\mu}\\right)^{\\sigma-1} \\exp\\left[-\\left(\\frac{y}{\\mu}\\right)^{\\sigma}\\right] $$</p> <p>where \\(y &gt; 0\\), \\(\\mu &gt; 0\\) is the scale parameter, and \\(\\sigma &gt; 0\\) is the shape parameter.</p> <p>Note</p> <p>This distribution corresponds to the WEI() distribution in GAMLSS.</p> <p>The parameterization matches scipy.stats.weibull_min with: - scale = mu (GAMLSS mu) - c = sigma (GAMLSS sigma, the shape parameter)</p> <p>Parameters:</p> <ul> <li> <code>scale_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>The link function for \\(\\mu\\). Defaults to Log().</p> </li> <li> <code>shape_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>The link function for \\(\\sigma\\). Defaults to Log().</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.Weibull.__init__","title":"__init__","text":"<pre><code>__init__(\n    scale_link: LinkFunction = Log(),\n    shape_link: LinkFunction = Log(),\n) -&gt; None\n</code></pre> <p>Initialize the Weibull distribution.</p> <p>Parameters:</p> <ul> <li> <code>scale_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>Link function for mu (scale). Defaults to Log().</p> </li> <li> <code>shape_link</code>               (<code>LinkFunction</code>, default:                   <code>Log()</code> )           \u2013            <p>Link function for sigma (shape). Defaults to Log().</p> </li> </ul>"},{"location":"distributions/#multivariate-distributions","title":"Multivariate Distributions","text":""},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseCholesky","title":"ondil.distributions.MultivariateNormalInverseCholesky","text":"<p>               Bases: <code>MultivariateDistributionMixin</code>, <code>Distribution</code></p> <p>The multivariate normal (Gaussian) distribution parameterized by the inverse Cholesky factor of the precision (inverse scale) matrix.</p> <p>The PDF of the multivariate normal distribution is given by: $$ p(y \\mid \\mu, L) = |L| \\cdot (2\\pi)^{-k/2} \\exp\\left(-\\frac{1}{2} (y - \\mu)^T (L L^T) (y - \\mu)\\right) $$</p> <p>where \\( k \\) is the dimensionality of the data, \\( \\mu \\) is the location parameter, and \\( L \\) is the inverse Cholesky factor of the precision matrix (so the precision is \\( L L^T \\)).</p>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseCholesky.set_theta_element","title":"set_theta_element  <code>staticmethod</code>","text":"<pre><code>set_theta_element(\n    theta: Dict, value: ndarray, param: int, k: int\n) -&gt; Dict\n</code></pre> <p>Sets an element of theta for parameter param and place k.</p> <p>Note</p> <p>This will mutate <code>theta</code>!</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Current fitted $      heta$</p> </li> <li> <code>value</code>               (<code>ndarray</code>)           \u2013            <p>Value to set</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Distribution parameter</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Flat element index \\(k\\)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Theta where element (param, k) is set to value.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseCholesky.dl1_dp1","title":"dl1_dp1","text":"<pre><code>dl1_dp1(y: ndarray, theta: Dict, param: int = 0)\n</code></pre> <p>Return the first derivatives wrt to the parameter.</p> <p>Note</p> <p>We expect the fitted L^-1)^T to be handed in matrix/cube form, i.e of shape n x d x d. But we return the derivatives in flat format.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>Y values of shape n x d</p> </li> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Dict with {0 : fitted mu, 1 : fitted (L^-1)^T}</p> </li> <li> <code>param</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Which parameter derivatives to return. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>derivative</code> (              <code>ndarray</code> )          \u2013            <p>The 1st derivatives.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseCholesky.dl2_dp2","title":"dl2_dp2","text":"<pre><code>dl2_dp2(\n    y: ndarray, theta: Dict, param: int = 0, clip=False\n) -&gt; np.ndarray\n</code></pre> <p>Return the second derivatives wrt to the parameter.</p> <p>Note</p> <p>We expect the fitted L^-1)^T to be handed in matrix/cube form, i.e of shape n x d x d. But we return the derivatives in flat format.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>Y values of shape n x d</p> </li> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Dict with {0 : fitted mu, 1 : fitted (L^-1)^T}</p> </li> <li> <code>param</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Which parameter derivatives to return. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>derivative</code> (              <code>ndarray</code> )          \u2013            <p>The 2nd derivatives.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseCholesky.param_conditional_likelihood","title":"param_conditional_likelihood","text":"<pre><code>param_conditional_likelihood(\n    y: ndarray, theta: Dict, eta: ndarray, param: int\n) -&gt; np.ndarray\n</code></pre> <p>Calulate the log-likelihood for (flat) eta for parameter (param) and theta for all other parameters.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>True values</p> </li> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Fitted theta.</p> </li> <li> <code>eta</code>               (<code>ndarray</code>)           \u2013            <p>Fitted eta.</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Param for which we take eta.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Log-likelihood.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseModifiedCholesky","title":"ondil.distributions.MultivariateNormalInverseModifiedCholesky","text":"<p>               Bases: <code>MultivariateDistributionMixin</code>, <code>Distribution</code></p> <p>The multivariate normal (Gaussian) distribution parameterized by the modified Cholesky decomposition of the precision (inverse scale) matrix.</p> <p>In the modified Cholesky decomposition, the precision matrix \\( \\Omega \\) is written as: $$ \\Omega = T^T D T $$ where \\( T \\) is a lower triangular matrix with ones on the diagonal, and \\( D \\) is a diagonal matrix with positive entries.</p> <p>The PDF of the multivariate normal distribution is then: $$ p(y \\mid \\mu, T, D) = |T| \\cdot |D|^{1/2} \\cdot (2\\pi)^{-k/2} \\exp\\left(-\\frac{1}{2} (y - \\mu)^T T^T D T (y - \\mu)\\right) $$ where \\( k \\) is the dimensionality of the data, \\( \\mu \\) is the mean vector, \\( T \\) and \\( D \\) are the modified Cholesky factors of the precision matrix.</p>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseModifiedCholesky.set_theta_element","title":"set_theta_element","text":"<pre><code>set_theta_element(\n    theta: Dict, value: ndarray, param: int, k: int\n) -&gt; Dict\n</code></pre> <p>Sets an element of theta for parameter param and place k.</p> <p>Note</p> <p>This will mutate <code>theta</code>!</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Current fitted $      heta$</p> </li> <li> <code>value</code>               (<code>ndarray</code>)           \u2013            <p>Value to set</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Distribution parameter</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Flat element index \\(k\\)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Theta where element (param, k) is set to value.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseModifiedCholesky.param_conditional_likelihood","title":"param_conditional_likelihood","text":"<pre><code>param_conditional_likelihood(\n    y: ndarray, theta: Dict, eta: ndarray, param: int\n) -&gt; np.ndarray\n</code></pre> <p>Calulate the log-likelihood for (flat) eta for parameter (param) and theta for all other parameters.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>True values</p> </li> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Fitted theta.</p> </li> <li> <code>eta</code>               (<code>ndarray</code>)           \u2013            <p>Fitted eta.</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Param for which we take eta.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Log-likelihood.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseLowRank","title":"ondil.distributions.MultivariateNormalInverseLowRank","text":"<p>               Bases: <code>MultivariateDistributionMixin</code>, <code>Distribution</code></p> <p>The multivariate normal (Gaussian) distribution parameterized by a low-rank precision matrix.</p> <p>The PDF of the multivariate normal distribution is given by: $$ p(y \\mid \\mu, D, V) = |D + V V^T|^{1/2} \\cdot (2\\pi)^{-k/2} \\exp\\left(-\\frac{1}{2} (y - \\mu)^T (D + V V^T) (y - \\mu)\\right) $$</p> <p>where \\( k \\) is the dimensionality of the data, \\( \\mu \\) is the location parameter, \\( D \\) is a diagonal matrix, and \\( V \\) is a low-rank matrix such that the precision is \\( D + V V^T \\).</p>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseLowRank.set_theta_element","title":"set_theta_element","text":"<pre><code>set_theta_element(\n    theta: Dict, value: ndarray, param: int, k: int\n) -&gt; Dict\n</code></pre> <p>Sets an element of theta for parameter param and place k.</p> <p>Note</p> <p>This will mutate <code>theta</code>!</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Current fitted $      heta$</p> </li> <li> <code>value</code>               (<code>ndarray</code>)           \u2013            <p>Value to set</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Distribution parameter</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Flat element index \\(k\\)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Theta where element (param, k) is set to value.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateNormalInverseLowRank.theta_to_scipy","title":"theta_to_scipy","text":"<pre><code>theta_to_scipy(\n    theta: Dict[int, ndarray],\n) -&gt; Dict[str, np.ndarray]\n</code></pre> <p>Map theta to <code>scipy</code> distribution parameters for the multivariate normal distribution.</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>Dict[int, ndarray]</code>)           \u2013            <p>Fitted / predicted theta.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[str, ndarray]</code>           \u2013            <p>Dict[str, np.ndarray]: Mapped predicted</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseCholesky","title":"ondil.distributions.MultivariateStudentTInverseCholesky","text":"<p>               Bases: <code>MultivariateDistributionMixin</code>, <code>Distribution</code></p> <p>The multivariate \\( t \\)-distribution parameterized by the inverse Cholesky factor of the precision (inverse scale) matrix.</p> <p>The PDF of the multivariate \\( t \\)-distribution is given by: $$ p(y \\mid \\mu, L, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + k}{2}\\right)}      {\\Gamma\\left(\\frac{\\nu}{2}\\right) \\left(\\pi \\nu\\right)^{k/2}} \\cdot |L| \\left(1 + \\frac{1}{\\nu} (y - \\mu)^T (L L^T) (y - \\mu)\\right)^{-\\frac{\\nu + k}{2}} $$</p> <p>where \\( k \\) is the dimensionality of the data, \\( \\mu \\) is the location parameter, \\( L \\) is the inverse Cholesky factor of the precision matrix (so the precision is \\( L L^T \\)), and \\( \\nu \\) is the degrees of freedom.</p>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseCholesky.set_theta_element","title":"set_theta_element  <code>staticmethod</code>","text":"<pre><code>set_theta_element(\n    theta: Dict, value: ndarray, param: int, k: int\n) -&gt; Dict\n</code></pre> <p>Sets an element of theta for parameter param and place k.</p> <p>Note</p> <p>This will mutate <code>theta</code>!</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Current fitted $      heta$</p> </li> <li> <code>value</code>               (<code>ndarray</code>)           \u2013            <p>Value to set</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Distribution parameter</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Flat element index \\(k\\)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Theta where element (param, k) is set to value.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseModifiedCholesky","title":"ondil.distributions.MultivariateStudentTInverseModifiedCholesky","text":"<p>               Bases: <code>MultivariateDistributionMixin</code>, <code>Distribution</code></p> <p>Multivariate Student's t distribution with modified Cholesky decomposition.</p> <p>The PDF of the multivariate \\( t \\)-distribution with precision matrix parameterized as \\( T^T D T \\) is: $$ p(y \\mid \\mu, D, T, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + k}{2}\\right)}      {\\Gamma\\left(\\frac{\\nu}{2}\\right) \\left(\\pi \\nu\\right)^{k/2}} \\cdot \\sqrt{\\det(D)} \\left(1 + \\frac{1}{\\nu} (y - \\mu)^T T^T D T (y - \\mu)\\right)^{-\\frac{\\nu + k}{2}} $$ where \\( k \\) is the dimensionality, \\( \\mu \\) is the location, \\( D \\) is a diagonal matrix, \\( T \\) is a lower triangular matrix, and \\( \\nu \\) is the degrees of freedom.</p>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseModifiedCholesky.__init__","title":"__init__","text":"<pre><code>__init__(\n    loc_link: LinkFunction = Identity(),\n    scale_link_1: LinkFunction = MatrixDiag(Log()),\n    scale_link_2: LinkFunction = MatrixDiagTril(\n        Identity(), Identity()\n    ),\n    tail_link: LinkFunction = LogShiftTwo(),\n    dof_guesstimate: float = 10,\n    use_gaussian_for_location: bool = False,\n)\n</code></pre> <p>Initializes the distribution with specified link functions and parameters. Args:     loc_link (LinkFunction, optional): Link function for the location parameter. Defaults to Identity().     scale_link_1 (LinkFunction, optional): Link function for the first scale parameter (diagonal). Defaults to MatrixDiag(Log()).     scale_link_2 (LinkFunction, optional): Link function for the second scale parameter (lower-triangular). Defaults to MatrixDiagTril(Identity(), Identity()).     tail_link (LinkFunction, optional): Link function for the tail (degrees of freedom) parameter. Defaults to LogShiftTwo().     dof_guesstimate (float, optional): Initial guess for the degrees of freedom. Defaults to 10.     use_gaussian_for_location (bool, optional): Whether to use a Gaussian approximation for the location parameter. Defaults to False. Attributes:     dof_guesstimate (float): Stores the initial guess for the degrees of freedom.     dof_independence (float): Large value to represent independence in degrees of freedom.     use_gaussian_for_location (bool): Indicates if a Gaussian is used for the location parameter.     _regularization_allowed (dict): Specifies which parameters allow regularization.</p>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseModifiedCholesky.set_theta_element","title":"set_theta_element","text":"<pre><code>set_theta_element(\n    theta: Dict, value: ndarray, param: int, k: int\n) -&gt; Dict\n</code></pre> <p>Sets an element of theta for parameter param and place k.</p> <p>Note</p> <p>This will mutate <code>theta</code>!</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Current fitted $      heta$</p> </li> <li> <code>value</code>               (<code>ndarray</code>)           \u2013            <p>Value to set</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Distribution parameter</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Flat element index \\(k\\)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Theta where element (param, k) is set to value.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseModifiedCholesky.param_conditional_likelihood","title":"param_conditional_likelihood","text":"<pre><code>param_conditional_likelihood(\n    y: ndarray, theta: Dict, eta: ndarray, param: int\n) -&gt; np.ndarray\n</code></pre> <p>Calulate the log-likelihood for (flat) eta for parameter (param) and theta for all other parameters.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>True values</p> </li> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Fitted theta.</p> </li> <li> <code>eta</code>               (<code>ndarray</code>)           \u2013            <p>Fitted eta.</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Param for which we take eta.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Log-likelihood.</p> </li> </ul>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseLowRank","title":"ondil.distributions.MultivariateStudentTInverseLowRank","text":"<p>               Bases: <code>MultivariateDistributionMixin</code>, <code>Distribution</code></p> <p>The multivariate \\( t \\)-distribution using a low-rank approximation (LRA) of the precision (inverse scale) matrix.</p> <p>The PDF of the multivariate \\( t \\)-distribution is given by: $$ p(y \\mid \\mu, D, V, \\nu) = \\frac{\\Gamma\\left(\\frac{\\nu + k}{2}\\right)}      {\\Gamma\\left(\\frac{\\nu}{2}\\right) \\left(\\pi \\nu\\right)^{k/2}} \\cdot \\frac{1}{\\sqrt{\\det(D + V V^T)}} \\left(1 + \\frac{1}{\\nu} (y - \\mu)^T (D + V V^T)^{-1} (y - \\mu)\\right)^{-\\frac{\\nu + k}{2}} $$</p> <p>where \\( k \\) is the dimensionality of the data, \\( \\mu \\) is the location parameter, \\( D \\) is a diagonal matrix, \\( V \\) is a low-rank matrix, and \\( \\nu \\) is the degrees of freedom.</p>"},{"location":"distributions/#ondil.distributions.MultivariateStudentTInverseLowRank.set_theta_element","title":"set_theta_element","text":"<pre><code>set_theta_element(\n    theta: Dict, value: ndarray, param: int, k: int\n) -&gt; Dict\n</code></pre> <p>Sets an element of theta for parameter param and place k.</p> <p>Note</p> <p>This will mutate <code>theta</code>!</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>Dict</code>)           \u2013            <p>Current fitted $      heta$</p> </li> <li> <code>value</code>               (<code>ndarray</code>)           \u2013            <p>Value to set</p> </li> <li> <code>param</code>               (<code>int</code>)           \u2013            <p>Distribution parameter</p> </li> <li> <code>k</code>               (<code>int</code>)           \u2013            <p>Flat element index \\(k\\)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict</code> (              <code>Dict</code> )          \u2013            <p>Theta where element (param, k) is set to value.</p> </li> </ul>"},{"location":"distributions/#base-class","title":"Base Class","text":""},{"location":"distributions/#ondil.base.Distribution","title":"ondil.base.Distribution","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"distributions/#ondil.base.Distribution.corresponding_gamlss","title":"corresponding_gamlss  <code>property</code>","text":"<pre><code>corresponding_gamlss: str | None\n</code></pre> <p>The name of the corresponding implementation in 'gamlss.dist' R package.</p>"},{"location":"distributions/#ondil.base.Distribution.parameter_names","title":"parameter_names  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>parameter_names: dict\n</code></pre> <p>Parameter name for each column of theta.</p>"},{"location":"distributions/#ondil.base.Distribution.parameter_shape","title":"parameter_shape  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>parameter_shape: dict\n</code></pre> <p>Parameter name for each column of theta.</p>"},{"location":"distributions/#ondil.base.Distribution.n_params","title":"n_params  <code>property</code>","text":"<pre><code>n_params: int\n</code></pre> <p>Each subclass must define 'n_params'.</p>"},{"location":"distributions/#ondil.base.Distribution.distribution_support","title":"distribution_support  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>distribution_support: Tuple[float, float]\n</code></pre> <p>The support of the distribution.</p>"},{"location":"distributions/#ondil.base.Distribution.parameter_support","title":"parameter_support  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>parameter_support: dict\n</code></pre> <p>The support of each parameter of the distribution.</p>"},{"location":"distributions/#ondil.base.Distribution.theta_to_params","title":"theta_to_params","text":"<pre><code>theta_to_params(theta: ndarray) -&gt; Tuple[np.ndarray, ...]\n</code></pre> <p>Take the fitted values and return tuple of vectors for distribution parameters.</p>"},{"location":"distributions/#ondil.base.Distribution.dl1_dp1","title":"dl1_dp1  <code>abstractmethod</code>","text":"<pre><code>dl1_dp1(\n    y: ndarray, theta: ndarray, param: int\n) -&gt; np.ndarray\n</code></pre> <p>Take the first derivative of the likelihood function with respect to the param.</p>"},{"location":"distributions/#ondil.base.Distribution.dl2_dp2","title":"dl2_dp2  <code>abstractmethod</code>","text":"<pre><code>dl2_dp2(\n    y: ndarray, theta: ndarray, param: int\n) -&gt; np.ndarray\n</code></pre> <p>Take the second derivative of the likelihood function with respect to the param.</p>"},{"location":"distributions/#ondil.base.Distribution.dl2_dpp","title":"dl2_dpp  <code>abstractmethod</code>","text":"<pre><code>dl2_dpp(\n    y: ndarray, theta: ndarray, params: Tuple[int, int]\n) -&gt; np.ndarray\n</code></pre> <p>Take the first derivative of the likelihood function with respect to both parameters.</p>"},{"location":"distributions/#ondil.base.Distribution.link_function","title":"link_function","text":"<pre><code>link_function(y: ndarray, param: int = 0) -&gt; np.ndarray\n</code></pre> <p>Apply the link function for param on y.</p>"},{"location":"distributions/#ondil.base.Distribution.link_inverse","title":"link_inverse","text":"<pre><code>link_inverse(y: ndarray, param: int = 0) -&gt; np.ndarray\n</code></pre> <p>Apply the inverse of the link function for param on y.</p>"},{"location":"distributions/#ondil.base.Distribution.link_function_derivative","title":"link_function_derivative","text":"<pre><code>link_function_derivative(\n    y: ndarray, param: int = 0\n) -&gt; np.ndarray\n</code></pre> <p>Apply the derivative of the link function for param on y.</p>"},{"location":"distributions/#ondil.base.Distribution.link_inverse_derivative","title":"link_inverse_derivative","text":"<pre><code>link_inverse_derivative(\n    y: ndarray, param: int = 0\n) -&gt; np.ndarray\n</code></pre> <p>Apply the derivative of the inverse link function for param on y.</p>"},{"location":"distributions/#ondil.base.Distribution.link_function_second_derivative","title":"link_function_second_derivative","text":"<pre><code>link_function_second_derivative(\n    y: ndarray, param: int = 0\n) -&gt; np.ndarray\n</code></pre> <p>Apply the second derivative of the link function for param on y.</p>"},{"location":"distributions/#ondil.base.Distribution.initial_values","title":"initial_values  <code>abstractmethod</code>","text":"<pre><code>initial_values(\n    y: ndarray,\n    param: int = 0,\n    axis: Optional[int | None] = None,\n) -&gt; np.ndarray\n</code></pre> <p>Calculate the initial values for the GAMLSS fit.</p>"},{"location":"distributions/#ondil.base.Distribution.quantile","title":"quantile","text":"<pre><code>quantile(q: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the quantile function for the given data.</p> <p>This is a alias for the <code>ppf</code> method.</p> <p>Parameters:</p> <ul> <li> <code>q</code>               (<code>ndarray</code>)           \u2013            <p>The quantiles to compute.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>The parameters of the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The quantiles corresponding to the given probabilities.</p> </li> </ul>"},{"location":"distributions/#ondil.base.Distribution.calculate_conditional_initial_values","title":"calculate_conditional_initial_values  <code>abstractmethod</code>","text":"<pre><code>calculate_conditional_initial_values(\n    y: ndarray, theta: ndarray, param: int\n) -&gt; np.ndarray\n</code></pre> <p>Calculate the conditional initial values for the GAMLSS fit.</p>"},{"location":"distributions/#ondil.base.Distribution.cdf","title":"cdf  <code>abstractmethod</code>","text":"<pre><code>cdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the cumulative distribution function (CDF) for the given data.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The data points at which to evaluate the CDF.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>The parameters of the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The CDF evaluated at the given data points.</p> </li> </ul>"},{"location":"distributions/#ondil.base.Distribution.pdf","title":"pdf  <code>abstractmethod</code>","text":"<pre><code>pdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the probability density function (PDF) for the given data points.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>An array of data points at which to evaluate the PDF.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>An array of parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: An array of PDF values corresponding to the data points in <code>y</code>.</p> </li> </ul>"},{"location":"distributions/#ondil.base.Distribution.pmf","title":"pmf  <code>abstractmethod</code>","text":"<pre><code>pmf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the probability mass function (PMF) for the given data points.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>An array of data points at which to evaluate the PDF.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>An array of parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: An array of PMF values corresponding to the data points in <code>y</code>.</p> </li> </ul>"},{"location":"distributions/#ondil.base.Distribution.ppf","title":"ppf  <code>abstractmethod</code>","text":"<pre><code>ppf(q: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Percent Point Function (Inverse of CDF).</p> <p>Parameters:</p> <ul> <li> <code>q</code>               (<code>ndarray</code>)           \u2013            <p>Quantiles.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>Distribution parameters.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The quantile corresponding to the given probabilities.</p> </li> </ul>"},{"location":"distributions/#ondil.base.Distribution.rvs","title":"rvs  <code>abstractmethod</code>","text":"<pre><code>rvs(size: int, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Generate random variates of given size and parameters.</p> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>The number of random variates to generate.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>The parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: A 2D array of random variates with shape (theta.shape[0], size).</p> </li> </ul>"},{"location":"distributions/#ondil.base.Distribution.logcdf","title":"logcdf  <code>abstractmethod</code>","text":"<pre><code>logcdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the log of the cumulative distribution function (CDF) for the given data points.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>An array of data points at which to evaluate the log CDF.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>An array of parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: An array of log CDF values corresponding to the data points in <code>y</code>.</p> </li> </ul>"},{"location":"distributions/#ondil.base.ScipyMixin","title":"ondil.base.ScipyMixin","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"distributions/#ondil.base.ScipyMixin.parameter_names","title":"parameter_names  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>parameter_names: dict\n</code></pre> <p>Parameter name for each column of theta.</p>"},{"location":"distributions/#ondil.base.ScipyMixin.scipy_dist","title":"scipy_dist  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>scipy_dist: rv_continuous\n</code></pre> <p>The names of the parameters in the scipy.stats distribution and the corresponding column in theta.</p>"},{"location":"distributions/#ondil.base.ScipyMixin.scipy_names","title":"scipy_names  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>scipy_names: Tuple[str]\n</code></pre> <p>The names of the parameters in the scipy.stats distribution and the corresponding column in theta.</p>"},{"location":"distributions/#ondil.base.ScipyMixin.theta_to_scipy_params","title":"theta_to_scipy_params","text":"<pre><code>theta_to_scipy_params(\n    theta: ndarray,\n) -&gt; Dict[str, np.ndarray]\n</code></pre> <p>Maps \\(\\theta\\) to the <code>scipy</code> parameters.</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>\\(\\theta\\) as estimated by <code>OnlineDistributionalRegression()</code> estimator</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If we don't define the <code>scipy_names</code> attribute.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict</code> (              <code>Dict[str, ndarray]</code> )          \u2013            <p>Dictionary that can be unrolled into scipy distribution class as in <code>st.some_dist(**return_value)</code></p> </li> </ul>"},{"location":"distributions/#ondil.base.ScipyMixin.logpmf","title":"logpmf","text":"<pre><code>logpmf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the log of the probability mass function (PMF) for the given data points.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>An array of data points at which to evaluate the log PMF.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>An array of parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: An array of log PMF values corresponding to the data points in <code>y</code>.</p> </li> </ul>"},{"location":"distributions/#ondil.base.ScipyMixin.logpdf","title":"logpdf","text":"<pre><code>logpdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the log of the probability density function (PDF) for the given data points.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>An array of data points at which to evaluate the log PDF.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>An array of parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: An array of log PDF values corresponding to the data points in <code>y</code>.</p> </li> </ul>"},{"location":"distributions/#ondil.base.ScipyMixin.logcdf","title":"logcdf","text":"<pre><code>logcdf(y: ndarray, theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the log of the cumulative distribution function (CDF) for the given data points.</p> <p>Parameters:</p> <ul> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>An array of data points at which to evaluate the log CDF.</p> </li> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>An array of parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: An array of log CDF values corresponding to the data points in <code>y</code>.</p> </li> </ul>"},{"location":"distributions/#ondil.base.ScipyMixin.mean","title":"mean","text":"<pre><code>mean(theta: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute the mean of the distribution for the given parameters.</p> <p>Parameters:</p> <ul> <li> <code>theta</code>               (<code>ndarray</code>)           \u2013            <p>An array of parameters for the distribution.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: An array of means corresponding to the parameters in <code>theta</code>.</p> </li> </ul>"},{"location":"estimators/","title":"Estimators","text":"<p>Estimator classes provide an <code>sklearn</code>-like API to fit, predict and update models with the accordingly named methods. <code>ondil</code> provides a variety of estimators for different tasks, including linear regression, regularized regression, and distributional regression. These estimators are designed to handle online learning scenarios, allowing for efficient updates as new data arrives. The following sections provide an overview of the available estimators in <code>ondil</code>. We first cover the linear models, followed by distributional regression models. </p>"},{"location":"estimators/#linear-models","title":"Linear Models","text":""},{"location":"estimators/#ondil.estimators.OnlineLinearModel","title":"ondil.estimators.OnlineLinearModel","text":"<p>               Bases: <code>OndilEstimatorMixin</code>, <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>Simple Online Linear Regression for the expected value.</p>"},{"location":"estimators/#ondil.estimators.OnlineLinearModel.__init__","title":"__init__","text":"<pre><code>__init__(\n    forget: float = 0.0,\n    scale_inputs: bool | ndarray = True,\n    fit_intercept: bool = True,\n    regularize_intercept: bool = False,\n    method: EstimationMethod | str = \"ols\",\n    ic: Literal[\"aic\", \"bic\", \"hqc\", \"max\"] = \"bic\",\n)\n</code></pre> <p>The basic linear model for many different estimation techniques.</p> <p>Parameters:</p> <ul> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Exponential discounting of old observations. Defaults to 0.</p> </li> <li> <code>scale_inputs</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to scale the \\(X\\) matrix. Defaults to True.</p> </li> <li> <code>fit_intercept</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add an intercept in the estimation. Defaults to True.</p> </li> <li> <code>regularize_intercept</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to regularize the intercept. Defaults to False.</p> </li> <li> <code>method</code>               (<code>EstimationMethod | str</code>, default:                   <code>'ols'</code> )           \u2013            <p>The estimation method. Can be a string or <code>EstimationMethod</code> class. Defaults to \"ols\".</p> </li> <li> <code>ic</code>               (<code>Literal['aic', 'bic', 'hqc', 'max']</code>, default:                   <code>'bic'</code> )           \u2013            <p>The information criteria for model selection. Defaults to \"bic\".</p> </li> </ul> <p>Raises:     ValueError: Will raise if you try to regularize the intercept, but not fit it.</p>"},{"location":"estimators/#ondil.estimators.OnlineLinearModel.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: ndarray,\n    sample_weight: Optional[ndarray] = None,\n) -&gt; OnlineLinearModel\n</code></pre> <p>Initial fit of the online regression model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The design matrix \\(X\\).</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The response vector \\(y\\).</p> </li> <li> <code>sample_weight</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>The sample weights. Defaults to None.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineLinearModel.update","title":"update","text":"<pre><code>update(\n    X: ndarray,\n    y: ndarray,\n    sample_weight: Optional[ndarray] = None,\n) -&gt; None\n</code></pre> <p>Update the regression model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The new row of the design matrix \\(X\\). Needs to be of shape 1 x n_features or n_obs_new x n_features.</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The new observation of \\(y\\). Needs to be the same shape as <code>X</code> or a single observation.</p> </li> <li> <code>sample_weight</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>The weight for the new observations. <code>None</code> implies all observations have weight 1. Defaults to None.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineLinearModel.score","title":"score","text":"<pre><code>score(X: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Calculate the coefficient of determination \\(R^2\\).</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The design matrix \\(X\\).</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>The response vector \\(y\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code> (              <code>float</code> )          \u2013            <p>The coefficient of determination \\(R^2\\).</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineLinearModel.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict using the optimal IC selection.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The design matrix \\(X\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The predictions for the optimal IC.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineLinearModel.predict_path","title":"predict_path","text":"<pre><code>predict_path(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict the full regularization path.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>The design matrix \\(X\\).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: The predictions for the full path.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineLasso","title":"ondil.estimators.OnlineLasso","text":"<p>               Bases: <code>OnlineLinearModel</code></p>"},{"location":"estimators/#ondil.estimators.OnlineLasso.__init__","title":"__init__","text":"<pre><code>__init__(\n    forget: float = 0,\n    scale_inputs: bool = True,\n    fit_intercept: bool = True,\n    regularize_intercept: bool = False,\n    ic: Literal[\"aic\", \"bic\", \"hqc\", \"max\"] = \"bic\",\n    early_stop: int = 0,\n    beta_lower_bound: ndarray | None = None,\n    beta_upper_bound: ndarray | None = None,\n    lambda_n: int = 100,\n    lambda_eps: float = 0.0001,\n    start_value: str = \"previous_fit\",\n    tolerance: float = 0.0001,\n    max_iterations: int = 1000,\n    selection: Literal[\"cyclic\", \"random\"] = \"cyclic\",\n)\n</code></pre> <p>Online LASSO estimator class.</p> <p>This class initializes the online linear regression fitted using LASSO. The estimator object provides three main methods, <code>estimator.fit(X, y)</code>, <code>estimator.update(X, y)</code> and <code>estimator.predict(X)</code>.</p> <p>Parameters:</p> <ul> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Exponential discounting of old observations. Defaults to 0.</p> </li> <li> <code>scale_inputs</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to scale the \\(X\\) matrix. Defaults to True.</p> </li> <li> <code>fit_intercept</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to add an intercept in the estimation. Defaults to True.</p> </li> <li> <code>regularize_intercept</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to regularize the intercept. Defaults to False.</p> </li> <li> <code>ic</code>               (<code>Literal['aic', 'bic', 'hqc', 'max']</code>, default:                   <code>'bic'</code> )           \u2013            <p>The information criteria for model selection. Defaults to \"bic\".</p> </li> <li> <code>early_stop</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Early stopping criterion. If we reach <code>early_stop</code> non-zero coefficients, we stop. Defaults to 0 (no early stopping).</p> </li> <li> <code>beta_lower_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Lower bounds for beta. Keep in mind the size of X and whether you want to fit an intercept. None corresponds to unconstrained estimation.Defaults to None.</p> </li> <li> <code>beta_upper_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Lower bounds for beta. Keep in mind the size of X and whether you want to fit an intercept. None corresponds to unconstrained estimation. Defaults to None.</p> </li> <li> <code>lambda_n</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Length of the regularization path. Defaults to 100.</p> </li> <li> <code>lambda_eps</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>The largest regularization is determined automatically such that the solution is fully regularized. The smallest regularization is taken as \\(\\varepsilon  \\lambda^\\max\\) and we will use an exponential grid. Defaults to 1e-4.</p> </li> <li> <code>start_value</code>               (<code>str</code>, default:                   <code>'previous_fit'</code> )           \u2013            <p>Whether to choose the previous fit or the previous regularization as start value. Defaults to 100.</p> </li> <li> <code>tolerance</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for breaking the CD. Defaults to 1e-4.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Max number of CD iterations. Defaults to 1000.</p> </li> <li> <code>selection</code>               (<code>Literal['cyclic', 'random']</code>, default:                   <code>'cyclic'</code> )           \u2013            <p>Whether to cycle through all coordinates in order or random. For large problems, random might increase convergence. Defaults to 100.</p> </li> </ul>"},{"location":"estimators/#online-distributional-regression","title":"Online Distributional Regression","text":""},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression","title":"ondil.estimators.OnlineDistributionalRegression","text":"<p>               Bases: <code>OndilEstimatorMixin</code>, <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>The online/incremental GAMLSS class.</p>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.__init__","title":"__init__","text":"<pre><code>__init__(\n    distribution: Distribution = Normal(),\n    equation: Dict[int, Union[str, ndarray, list]] = None,\n    forget: float | Dict[int, float] = 0.0,\n    method: Union[\n        str,\n        EstimationMethod,\n        Dict[int, str],\n        Dict[int, EstimationMethod],\n    ] = \"ols\",\n    scale_inputs: bool | ndarray = True,\n    fit_intercept: Union[bool, Dict[int, bool]] = True,\n    regularize_intercept: Union[\n        bool, Dict[int, bool]\n    ] = False,\n    ic: Union[str, Dict] = \"aic\",\n    model_selection: Literal[\n        \"local_rss\", \"global_ll\"\n    ] = \"local_rss\",\n    prefit_initial: int = 0,\n    prefit_update: int = 0,\n    step_size: float | Dict[int, float] = 1.0,\n    verbose: int = 0,\n    debug: bool = False,\n    param_order: ndarray | None = None,\n    cautious_updates: bool = False,\n    cond_start_val: bool = False,\n    max_it_outer: int = 30,\n    max_it_inner: int = 30,\n    abs_tol_outer: float = 0.001,\n    abs_tol_inner: float = 0.001,\n    rel_tol_outer: float = 1e-05,\n    rel_tol_inner: float = 1e-05,\n    min_it_outer: int = 1,\n) -&gt; OnlineDistributionalRegression\n</code></pre> <p>The <code>OnlineDistributionalRegression()</code> provides the fit, update and predict methods for linear parametric GAMLSS models.</p> <p>For a response variable \\(Y\\) which is distributed according to the distribution \\(\\mathcal{F}(\\theta)\\) with the distribution parameters \\(\\theta\\), we model:</p> \\[g_k(\\theta_k) = \\eta_k = X_k\\beta_k\\] <p>where \\(g_k(\\cdot)\\) is a link function, which ensures that the predicted distribution parameters are in a sensible range (we don't want, e.g. negative standard deviations), and \\(\\eta_k\\) is the predictor (on the space of the link function). The model is fitted using iterative re-weighted least squares (IRLS).</p> <p>Tips and Tricks</p> <p>If you're facing issues with non-convergence and/or matrix inversion problems, please enable the <code>debug</code> mode and increase the logging level by increasing <code>verbose</code>. In debug mode, the estimator will save the weights, working vectors, derivatives each iteration in a according dictionary, i.e. self._debug_weights. The keys are composed of a tuple of ints of <code>(parameter, outer_iteration, inner_iteration)</code>. Very small and/or very large weights (implicitly second derivatives) can be a sign that either start values are not chosen appropriately or that the distributional assumption does not fit the data well.</p> <p>Debug Mode</p> <p>Please don't use debug more for production models since it saves the <code>X</code> matrix and its scaled counterpart, so you will get large estimator objects.</p> <p>Conditional start values <code>cond_start_val=False</code></p> <p>The <code>cond_start_val</code> parameter is considered experimental and may not work as expected.</p> <p>Cautious updates <code>cautious_updates=True</code></p> <p>The <code>cautious_updates</code> parameter is considered experimental and may not work as expected.</p> <p>Parameters:</p> <ul> <li> <code>distribution</code>               (<code>Distribution</code>, default:                   <code>Normal()</code> )           \u2013            <p>The parametric distribution to use for modeling the response variable.</p> </li> <li> <code>equation</code>               (<code>Dict[int, Union[str, ndarray, list]]</code>, default:                   <code>None</code> )           \u2013            <p>The modeling equation for each distribution parameter. The dictionary should map parameter indices to either the strings <code>'all'</code>, <code>'intercept'</code>, a numpy array of column indices, or a list of column names. Defaults to None, which uses all covariates for the first parameter and intercepts for others.</p> </li> <li> <code>forget</code>               (<code>float | Dict[int, float]</code>, default:                   <code>0.0</code> )           \u2013            <p>The forget factor for exponential weighting of past observations. Can be a single float for all parameters or a dictionary mapping parameter indices to floats. Defaults to 0.0.</p> </li> <li> <code>method</code>               (<code>str | EstimationMethod | Dict[int, str] | Dict[int, EstimationMethod]</code>, default:                   <code>'ols'</code> )           \u2013            <p>The estimation method for each parameter. Can be a string, EstimationMethod, or a dictionary mapping parameter indices. Defaults to \"ols\".</p> </li> <li> <code>scale_inputs</code>               (<code>bool | ndarray</code>, default:                   <code>True</code> )           \u2013            <p>Whether to scale the input features. Can be a boolean or a numpy array specifying scaling per feature. Defaults to True.</p> </li> <li> <code>fit_intercept</code>               (<code>bool | Dict[int, bool]</code>, default:                   <code>True</code> )           \u2013            <p>Whether to fit an intercept for each parameter. Can be a boolean or a dictionary mapping parameter indices. Defaults to True.</p> </li> <li> <code>regularize_intercept</code>               (<code>bool | Dict[int, bool]</code>, default:                   <code>False</code> )           \u2013            <p>Whether to regularize the intercept for each parameter. Can be a boolean or a dictionary mapping parameter indices. Defaults to False.</p> </li> <li> <code>ic</code>               (<code>str | Dict</code>, default:                   <code>'aic'</code> )           \u2013            <p>Information criterion for model selection (e.g., \"aic\", \"bic\"). Can be a string or a dictionary mapping parameter indices. Defaults to \"aic\".</p> </li> <li> <code>model_selection</code>               (<code>Literal['local_rss', 'global_ll']</code>, default:                   <code>'local_rss'</code> )           \u2013            <p>Model selection strategy. \"local_rss\" selects based on local residual sum of squares, \"global_ll\" uses global log-likelihood. Defaults to \"local_rss\".</p> </li> <li> <code>prefit_initial</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of initial outer iterations with only one inner iteration (for stabilization). Defaults to 0.</p> </li> <li> <code>prefit_update</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Number of initial outer iterations with only one inner iteration during updates. Defaults to 0.</p> </li> <li> <code>step_size</code>               (<code>float | Dict[int, float]</code>, default:                   <code>1.0</code> )           \u2013            <p>Step size for parameter updates. Can be a float or a dictionary mapping parameter indices. Defaults to 1.0.</p> </li> <li> <code>verbose</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Verbosity level for logging. 0 = silent, 1 = high-level, 2 = per-parameter, 3 = per-iteration. Defaults to 0.</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable debug mode. Debug mode will save additional data to the estimator object. Currently, we save</p> <pre><code>* self._debug_X_dict\n* self._debug_X_scaled\n* self._debug_weights\n* self._debug_working_vectors\n* self._debug_dl1dlp1\n* self._debug_dl2dlp2\n* self._debug_eta\n* self._debug_fv\n* self._debug_coef\n* self._debug_coef_path\n</code></pre> <p>to the the estimator. Debug mode works in batch and online settings. Note that debug mode is not recommended for production use. Defaults to False.</p> </li> <li> <code>param_order</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Order in which to fit the distribution parameters. Defaults to None (natural order).</p> </li> <li> <code>cautious_updates</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use smaller step sizes and more iterations when new data are outliers. Defaults to False.</p> </li> <li> <code>cond_start_val</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, use conditional start values for parameters (experimental). Defaults to False.</p> </li> <li> <code>max_it_outer</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>Maximum number of outer iterations for the fitting algorithm. Defaults to 30.</p> </li> <li> <code>max_it_inner</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>Maximum number of inner iterations for the fitting algorithm. Defaults to 30.</p> </li> <li> <code>abs_tol_outer</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Absolute tolerance for convergence in the outer loop. Defaults to 1e-3.</p> </li> <li> <code>abs_tol_inner</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Absolute tolerance for convergence in the inner loop. Defaults to 1e-3.</p> </li> <li> <code>rel_tol_outer</code>               (<code>float</code>, default:                   <code>1e-05</code> )           \u2013            <p>Relative tolerance for convergence in the outer loop. Defaults to 1e-5.</p> </li> <li> <code>rel_tol_inner</code>               (<code>float</code>, default:                   <code>1e-05</code> )           \u2013            <p>Relative tolerance for convergence in the inner loop. Defaults to 1e-5.</p> </li> <li> <code>min_it_outer</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Minimum number of outer iterations before checking for convergence. Defaults to 1.</p> </li> </ul> <p>Attributes:</p> <ul> <li> <code>distribution</code>               (<code>Distribution</code>)           \u2013            <p>The distribution used for modeling.</p> </li> <li> <code>equation</code>               (<code>Dict[int, Union[str, ndarray, list]]</code>)           \u2013            <p>The modeling equation for each distribution parameter.</p> </li> <li> <code>forget</code>               (<code>Dict[int, float]</code>)           \u2013            <p>Forget factor for each distribution parameter.</p> </li> <li> <code>fit_intercept</code>               (<code>Dict[int, bool]</code>)           \u2013            <p>Whether to fit an intercept for each parameter.</p> </li> <li> <code>regularize_intercept</code>               (<code>Dict[int, bool]</code>)           \u2013            <p>Whether to regularize the intercept for each parameter.</p> </li> <li> <code>ic</code>               (<code>Dict[int, str]</code>)           \u2013            <p>Information criterion for model selection for each parameter.</p> </li> <li> <code>method</code>               (<code>Dict[int, EstimationMethod]</code>)           \u2013            <p>Estimation method for each parameter.</p> </li> <li> <code>scale_inputs</code>               (<code>bool | ndarray</code>)           \u2013            <p>Whether to scale the input features.</p> </li> <li> <code>param_order</code>               (<code>ndarray | None</code>)           \u2013            <p>Order in which to fit the distribution parameters.</p> </li> <li> <code>n_observations_</code>               (<code>float</code>)           \u2013            <p>Total number of observations used for fitting.</p> </li> <li> <code>n_training_</code>               (<code>Dict[int, int]</code>)           \u2013            <p>Effective training length for each distribution parameter.</p> </li> <li> <code>n_features_</code>               (<code>Dict[int, int]</code>)           \u2013            <p>Number of features used for each distribution parameter.</p> </li> <li> <code>coef_</code>               (<code>ndarray</code>)           \u2013            <p>Coefficients for the fitted model, shape (n_params, n_features).</p> </li> <li> <code>coef_path_</code>               (<code>ndarray</code>)           \u2013            <p>Coefficients path for the fitted model, shape (n_params, n_iterations, n_features). Only available if <code>method</code> is a path-based method like LASSO.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>OnlineDistributionalRegression</code> (              <code>OnlineDistributionalRegression</code> )          \u2013            <p>The OnlineDistributionalRegression instance.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: ndarray,\n    sample_weight: Optional[ndarray] = None,\n) -&gt; OnlineDistributionalRegression\n</code></pre> <p>Fit the online GAMLSS model.</p> <p>This method initializes the model with the given covariate data matrix \\(X\\) and response variable \\(Y\\).</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Covariate data matrix \\(X\\).</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>Response variable \\(Y\\).</p> </li> <li> <code>sample_weight</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>User-defined sample weights. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>OnlineDistributionalRegression</code> (              <code>OnlineDistributionalRegression</code> )          \u2013            <p>The fitted OnlineDistributionalRegression instance.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the equation is not specified correctly.</p> </li> <li> <code>OutOfSupportError</code>             \u2013            <p>If the values of \\(y\\) are below or above the distribution's support.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.update","title":"update","text":"<pre><code>update(\n    X: ndarray,\n    y: ndarray,\n    sample_weight: Optional[ndarray] = None,\n)\n</code></pre> <p>Update the fit for the online GAMLSS Model.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Covariate data matrix \\(X\\).</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>Response variable \\(Y\\).</p> </li> <li> <code>sample_weight</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>User-defined sample weights. Defaults to None (all observations have the same weight).</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.predict","title":"predict","text":"<pre><code>predict(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Predict the mean of the response distribution.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Covariate matrix \\(X\\). Shape should be (n_samples, n_features).</p> </li> </ul> <p>Raises:     NotFittedError: If the model is not fitted yet.</p> <p>Returns:</p> <ul> <li> <code>Predictions</code> (              <code>ndarray</code> )          \u2013            <p>Predictions</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.predict_median","title":"predict_median","text":"<pre><code>predict_median(X: ndarray)\n</code></pre> <p>Predict the median of the distribution.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Covariate matrix \\(X\\). Shape should be (n_samples, n_features).</p> </li> </ul> <p>Raises:     NotFittedError: If the model is not fitted yet.</p> <p>Returns:</p> <ul> <li> <code>Predictions</code> (              <code>ndarray</code> )          \u2013            <p>Predicted median of the distribution. Shape will be (n_samples,).</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.predict_distribution_parameters","title":"predict_distribution_parameters","text":"<pre><code>predict_distribution_parameters(\n    X: ndarray,\n    what: str = \"response\",\n    return_contributions: bool = False,\n) -&gt; np.ndarray\n</code></pre> <p>Predict the distibution parameters given input data.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Design matrix.</p> </li> <li> <code>what</code>               (<code>str</code>, default:                   <code>'response'</code> )           \u2013            <p>Predict the response or the link. Defaults to \"response\". Remember the  GAMLSS models \\(g(\\theta) = X^T\\beta\\). Predict <code>\"link\"</code> will output \\(X^T\\beta\\), predict <code>\"response\"</code> will output \\(g^{-1}(X^T\\beta)\\). Usually, you want predict = <code>\"response\"</code>.</p> </li> <li> <code>return_contributions</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to return a <code>Tuple[prediction, contributions]</code> where the contributions of the individual covariates for each distribution parameter's predicted value is specified. Defaults to False.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>Raises if <code>what</code> is not in <code>[\"link\", \"response\"]</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Predictions</code> (              <code>ndarray</code> )          \u2013            <p>Predicted values for the distribution of shape (n_samples, n_params) where n_params is the number of distribution parameters.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.predict_quantile","title":"predict_quantile","text":"<pre><code>predict_quantile(\n    X: ndarray, quantile: float | ndarray\n) -&gt; np.ndarray\n</code></pre> <p>Predict the quantile(s) of the distribution.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Covariate matrix \\(X\\). Shape should be (n_samples, n_features).</p> </li> <li> <code>quantile</code>               (<code>float | ndarray</code>)           \u2013            <p>Quantile(s) to predict.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Predicted quantile(s) of the distribution. Shape will be (n_samples, n_quantiles).</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.OnlineDistributionalRegression.get_debug_information","title":"get_debug_information","text":"<pre><code>get_debug_information(\n    variable: str = \"coef\",\n    param: int = 0,\n    it_outer: int = 1,\n    it_inner: int = 1,\n)\n</code></pre> <p>Get debug information for a specific variable, parameter, outer iteration and inner iteration.</p> <p>We currently support the following variables:</p> <ul> <li>\"X_dict\": The design matrix for the distribution parameter.</li> <li>\"X_scaled\": The scaled design matrix.</li> <li>\"weights\": The sample weights for the distribution parameter.</li> <li>\"working_vectors\": The working vectors for the distribution parameter.</li> <li>\"dl1dlp1\": The first derivative of the log-likelihood with respect to the distribution parameter.</li> <li>\"dl2dlp2\": The second derivative of the log-likelihood with respect to the distribution parameter.</li> <li>\"eta\": The linear predictor for the distribution parameter.</li> <li>\"fv\": The fitted values for the distribution parameter.</li> <li>\"dv\": The deviance for the distribution parameter.</li> <li>\"coef\": The coefficients for the distribution parameter.</li> <li>\"coef_path\": The coefficients path for the distribution parameter.</li> </ul> <p>Parameters:</p> <ul> <li> <code>variable</code>               (<code>str</code>, default:                   <code>'coef'</code> )           \u2013            <p>The variable to get debug information for. Defaults to \"coef\".</p> </li> <li> <code>param</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>The distribution parameter to get debug information for. Defaults to 0.</p> </li> <li> <code>it_outer</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The outer iteration to get debug information for. Defaults to 1.</p> </li> <li> <code>it_inner</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>The inner iteration to get debug information for. Defaults to 1.</p> </li> </ul> <p>Returns:     Any: The debug information for the specified variable, parameter, outer iteration and inner iteration. Raises:     ValueError: If debug mode is not enabled.</p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath","title":"ondil.estimators.MultivariateOnlineDistributionalRegressionPath","text":"<p>               Bases: <code>OndilEstimatorMixin</code>, <code>RegressorMixin</code>, <code>MultiOutputMixin</code>, <code>BaseEstimator</code></p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.__init__","title":"__init__","text":"<pre><code>__init__(\n    distribution: Distribution = MultivariateNormalInverseCholesky(),\n    equation: Dict | None = None,\n    forget: float | Dict = 0.0,\n    learning_rate: float = 0.0,\n    fit_intercept: bool = True,\n    regularize_intercept: bool = False,\n    scale_inputs: bool = True,\n    verbose: int = 1,\n    method: (\n        Literal[\"ols\", \"lasso\"]\n        | Dict[int, Literal[\"ols\", \"lasso\"]]\n    ) = \"ols\",\n    ic: Literal[\"ll\", \"aic\", \"bic\", \"hqc\", \"max\"] = \"aic\",\n    iteration_along_diagonal: bool = False,\n    approx_fast_model_selection: bool = True,\n    max_regularisation_size: int | None = None,\n    early_stopping: bool = True,\n    early_stopping_criteria: Literal[\n        \"ll\", \"aic\", \"bic\", \"hqc\", \"max\"\n    ] = \"aic\",\n    early_stopping_abs_tol: float = 0.001,\n    early_stopping_rel_tol: float = 0.001,\n    weight_delta: float | Dict[int, float] = 1.0,\n    max_iterations_inner: int = 10,\n    max_iterations_outer: int = 10,\n    overshoot_correction: Optional[Dict[int, float]] = None,\n    dampen_estimation: bool | int = False,\n    debug: bool = False,\n    rel_tol_inner: float = 0.001,\n    abs_tol_inner: float = 0.001,\n    rel_tol_outer: float = 0.001,\n    abs_tol_outer: float = 0.001,\n)\n</code></pre> <p>Initialize the Online Multivariate Distributional Regression Path Estimator.</p> <p>This estimator fits a multivariate distribution to the data using an online approach. It regularizes the scale matrix by estimating it sequentially from an \"independence\" configuration to a full covariance matrix, using the AD-R regularization scheme. Supports various regression methods, model selection criteria, early stopping, and input scaling.</p> <p>Parameters:</p> <ul> <li> <code>distribution</code>               (<code>Distribution</code>, default:                   <code>MultivariateNormalInverseCholesky()</code> )           \u2013            <p>The multivariate distribution to fit. Defaults to MultivariateNormalInverseCholesky().</p> </li> <li> <code>equation</code>               (<code>Dict | None</code>, default:                   <code>None</code> )           \u2013            <p>Dictionary specifying the regression equation. Defaults to None.</p> </li> <li> <code>forget</code>               (<code>float | Dict</code>, default:                   <code>0.0</code> )           \u2013            <p>Forgetting factor for online updates, can be a float or per-dimension dict. Defaults to 0.0.</p> </li> <li> <code>learning_rate</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>Learning rate for online updates. Defaults to 0.0.</p> </li> <li> <code>fit_intercept</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to fit an intercept term. Defaults to True.</p> </li> <li> <code>regularize_intercept</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to regularize the intercept term. Defaults to False.</p> </li> <li> <code>scale_inputs</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Whether to scale input features. Defaults to True.</p> </li> <li> <code>verbose</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Verbosity level for logging. Defaults to 1.</p> </li> <li> <code>method</code>               (<code>Literal['ols', 'lasso'] | Dict[int, Literal['ols', 'lasso']]</code>, default:                   <code>'ols'</code> )           \u2013            <p>Regression method(s) to use, either \"ols\", \"lasso\", or per-dimension dict. Defaults to \"ols\".</p> </li> <li> <code>ic</code>               (<code>Literal['ll', 'aic', 'bic', 'hqc', 'max']</code>, default:                   <code>'aic'</code> )           \u2013            <p>Information criterion for model selection. Defaults to \"aic\".</p> </li> <li> <code>iteration_along_diagonal</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>If True, iterates regularization along the diagonal of the scale matrix. Defaults to False.</p> </li> <li> <code>approx_fast_model_selection</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>If True, uses approximate fast model selection. Defaults to True.</p> </li> <li> <code>max_regularisation_size</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Maximum size for regularization path. Defaults to None.</p> </li> <li> <code>early_stopping</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Enables early stopping during regularization. Defaults to True.</p> </li> <li> <code>early_stopping_criteria</code>               (<code>Literal['ll', 'aic', 'bic', 'hqc', 'max']</code>, default:                   <code>'aic'</code> )           \u2013            <p>Criterion for early stopping. Defaults to \"aic\".</p> </li> <li> <code>early_stopping_abs_tol</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Absolute tolerance for early stopping. Defaults to 0.001.</p> </li> <li> <code>early_stopping_rel_tol</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Relative tolerance for early stopping. Defaults to 0.001.</p> </li> <li> <code>weight_delta</code>               (<code>float | Dict[int, float]</code>, default:                   <code>1.0</code> )           \u2013            <p>Step size for weight updates, can be float or per-dimension dict. Defaults to 1.0.</p> </li> <li> <code>max_iterations_inner</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum number of inner iterations. Defaults to 10.</p> </li> <li> <code>max_iterations_outer</code>               (<code>int</code>, default:                   <code>10</code> )           \u2013            <p>Maximum number of outer iterations. Defaults to 10.</p> </li> <li> <code>overshoot_correction</code>               (<code>Optional[Dict[int, float]]</code>, default:                   <code>None</code> )           \u2013            <p>Correction factors for overshooting during updates. Defaults to None.</p> </li> <li> <code>dampen_estimation</code>               (<code>bool | int</code>, default:                   <code>False</code> )           \u2013            <p>If True or int, dampens estimation updates. Defaults to False.</p> </li> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enables debug mode for additional logging. Defaults to False.</p> </li> <li> <code>rel_tol_inner</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Relative tolerance for convergence in inner loop. Defaults to 1e-3.</p> </li> <li> <code>abs_tol_inner</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Absolute tolerance for convergence in inner loop. Defaults to 1e-3.</p> </li> <li> <code>rel_tol_outer</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Relative tolerance for convergence in outer loop. Defaults to 1e-3.</p> </li> <li> <code>abs_tol_outer</code>               (<code>float</code>, default:                   <code>0.001</code> )           \u2013            <p>Absolute tolerance for convergence in outer loop. Defaults to 1e-3.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray, y: ndarray\n) -&gt; MultivariateOnlineDistributionalRegressionPath\n</code></pre> <p>Fit the estimator to the data.</p> <p>Note</p> <p>The response \\(y\\) must be a multivariate response variable, i.e., a 2D array where each column represents a different response variable and we expect at least a bivariate response. Hence, passing an <code>(n, 1)</code> array for y does not work!</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Covariate or feature matrix.</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>Multivariate response variable.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>estimator</code> (              <code>MultivariateOnlineDistributionalRegressionPath</code> )          \u2013            <p>Returns the fitted estimator.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.count_coef_to_be_fitted","title":"count_coef_to_be_fitted","text":"<pre><code>count_coef_to_be_fitted(\n    outer_iteration: int,\n    inner_iteration: int,\n    adr: int,\n    param: int,\n    k: int,\n)\n</code></pre> <p>Count all coefficients that should be fitted.</p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.predict","title":"predict","text":"<pre><code>predict(\n    X: Optional[ndarray] = None,\n) -&gt; Dict[int, np.ndarray]\n</code></pre> <p>Predict the location parameter.</p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.predict--parameters","title":"Parameters","text":"<p>X : np.ndarray, optional     The input data. If None, will use a default value of ones.</p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.predict_distribution_parameters","title":"predict_distribution_parameters","text":"<pre><code>predict_distribution_parameters(\n    X: Optional[ndarray] = None,\n) -&gt; Dict[int, np.ndarray]\n</code></pre> <p>Predict the distribution parameters.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Covariate or feature matrix. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[int, ndarray]</code>           \u2013            <p>Dict[int, np.ndarray]: Return the predicted distribution parameters as a dictionary.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.predict_all_adr","title":"predict_all_adr","text":"<pre><code>predict_all_adr(\n    X: Optional[ndarray] = None,\n) -&gt; Dict[int, np.ndarray]\n</code></pre> <p>Predict all regualarized fits in one go.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>Optional[ndarray]</code>, default:                   <code>None</code> )           \u2013            <p>Covariate or feature matrix. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Dict[int, ndarray]</code>           \u2013            <p>Dict[int, np.ndarray]: Return the predicted distribution parameters for all AD-R steps as a dictionary. The structure is</p> </li> <li> <code>Dict[int, ndarray]</code>           \u2013            <p>{adr_step: {param: np.ndarray}} where <code>adr_step</code> is the AD-R step index and <code>param</code> is the distribution parameter index.</p> </li> </ul>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.partial_fit","title":"partial_fit","text":"<pre><code>partial_fit(X: ndarray, y: ndarray)\n</code></pre> <p>Align ondil with the scikit-learn API for partial fitting.</p> <p>The first partial fit will call <code>fit</code>, and subsequent calls will call <code>update</code>. Allows furthermore to use the sklearn testing framework.</p> <p>Overwrites the base class method to avoid sample_weights. This estimator does not support sample weights.</p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.partial_fit--parameters","title":"Parameters","text":"<p>X : np.ndarray     The input data. y : np.ndarray     The target values. Returns</p> <p>self : Estimator     The fitted estimator.</p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.update","title":"update","text":"<pre><code>update(\n    X: ndarray, y: ndarray\n) -&gt; MultivariateOnlineDistributionalRegressionPath\n</code></pre> <p>Updates the estimator with new observations.</p> <p>This method validates the input data, updates internal counters for the number of observations, recalculates the effective training length based on the learning rate, and updates the model's likelihood. It also scales the input features, stores previous states for model selection and likelihood, and performs an outer update step with the scaled data.</p>"},{"location":"estimators/#ondil.estimators.MultivariateOnlineDistributionalRegressionPath.update--parameters","title":"Parameters","text":"<p>X : np.ndarray     Input feature matrix of shape (n_samples, n_features). y : np.ndarray     Target values of shape (n_samples, n_outputs). Returns</p> <p>self : object     Returns the updated estimator instance.</p>"},{"location":"estimators_and_methods/","title":"The <code>Estimator()</code> and <code>EstimationMethod()</code> classes","text":""},{"location":"estimators_and_methods/#overview","title":"Overview","text":"<p>Our package separates <code>Estimator</code> classes and <code>EstimationMethod</code> classes in the design. An <code>Estimator</code> is a python object that provides the user interface to set-up, fit, update and predict models. <code>EstimationMethod</code> classes are concerned with the estimation of the model coefficients (or weights). This page briefly explains the separation and options provided by it using the <code>OnlineLinearModel()</code> class.</p> <p>Estimators are your bread and butter partner for modelling. They provide the methods:</p> <ul> <li><code>Estimator().fit(X, y)</code></li> <li><code>Estimator().update(X, y)</code></li> <li><code>Estimator().predict_distribution_parameters(X, y)</code></li> <li><code>Estimator().predict(X)</code></li> </ul> <p>which one commonly uses for modelling.</p> <p>Each estimator is initialized by choosing an estimation method passed to the <code>method</code> parameter, if the method is not explicitly called in the name of the estimator (like in the <code>OnlineLasso()</code>). The <code>method</code> accepts either a <code>string</code>, or an <code>EstimationMethod()</code> instance.</p>"},{"location":"estimators_and_methods/#example","title":"Example","text":"<p>Let's return to the aforementioned example: We want to fit a simple linear model. We can estimate the parameters either using ordinary least squares (OLS) or using coordinate descent, minimizing the LASSO penalised loss.</p>"},{"location":"estimators_and_methods/#ordinary-least-squares","title":"Ordinary Least Squares","text":"<p>First, we start with OLS:</p> <pre><code># Set up packages and\nfrom ondil.estimators.online_linear_model import OnlineLinearModel\nfrom ondil.methods import LassoPath, OrdinaryLeastSquares\nfrom sklearn.datasets import load_diabetes\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Get data\nX, y = load_diabetes(return_X_y=True)\n\nfit_intercept = False\nscale_inputs = True\n\n# This is the Estimator Class\nmodel = OnlineLinearModel(\n    method=\"ols\", \n    fit_intercept=fit_intercept, \n    scale_inputs=scale_inputs,\n)\nmodel.fit(X[:-10, :], y[:-10])\nmodel.update(X[-10:, :], y[-10:])\n\n# This is equivalent\nmodel = OnlineLinearModel(\n    method=OrdinaryLeastSquares(), \n    fit_intercept=fit_intercept, \n    scale_inputs=scale_inputs,\n)\nmodel.fit(X[:-10, :], y[:-10])\nmodel.update(X[-10:, :], y[-10:])\n</code></pre> <p>Since ordinary least squares is a pretty simple method, it does not have a lot of parameters. However, if we look at LASSO, things change, because now we can actually play with the parameters.</p>"},{"location":"estimators_and_methods/#lasso-and-the-lassopath","title":"LASSO and the <code>LassoPath()</code>","text":"<p>The <code>LassoPath()</code> estimates the coefficients using coordinate descent along a path of decreasing regularization strength. In this example, we will change some of the parameters of the estimation.</p> <p>The <code>LassoPath()</code> has for example the following parameters</p> <ul> <li><code>lambda_n</code> which defines the length of the regularization path.</li> <li><code>beta_lower_bounds</code> which provides the option to place a lower bound on the coefficients/weights.</li> </ul> <p>Let's have a look at a basic LASSO-estimated model:</p> <pre><code>model = OnlineLinearModel(\n    method=\"lasso\",\n    fit_intercept=fit_intercept,\n    scale_inputs=scale_inputs,\n)\nmodel.fit(X[:-10, :], y[:-10])\nplt.plot(model.beta_path)\nplt.show(block=False)\nprint(model.beta)\n\n# Equivalent, we can do:\n\nmodel = OnlineLinearModel(\n    method=LassoPath(),\n    fit_intercept=fit_intercept,\n    scale_inputs=scale_inputs,\n)\nmodel.fit(X[:-10, :], y[:-10])\nplt.plot(model.beta_path)\nplt.show(block=False)\nprint(model.beta)\n</code></pre> <p>Now we want to change the parameters:</p> <pre><code>estimation_method = LassoPath(\n    lambda_n=10,  # Only fit ten lambdas\n    beta_lower_bound=np.zeros(\n        X.shape[1] + fit_intercept\n    ),  # all positive parameters\n)\n\nmodel = OnlineLinearModel(\n    method=estimation_method,\n    fit_intercept=fit_intercept,\n    scale_inputs=scale_inputs,\n)\nmodel.fit(X[:-10, :], y[:-10])\nplt.plot(model.beta_path)\nplt.show(block=False)\nprint(model.beta)\n</code></pre> <p>And we see that the coefficient path is both shorter and non-negative.</p>"},{"location":"gram/","title":"Gramian Matrix","text":"<p>The Gramian</p>"},{"location":"gram/#api-reference","title":"API Reference","text":""},{"location":"gram/#ondil.gram.init_forget_vector","title":"ondil.gram.init_forget_vector","text":"<pre><code>init_forget_vector(forget: float, size: int) -&gt; np.ndarray\n</code></pre> <p>Initialise an exponentially discounted vector of weights.</p> <p>Recursively initialise a vector of exponentially discounted weights of <code>size</code> N.</p> <p>The weight for \\(n\\)-th observation is defined as \\((1 - \\text{forget})^{(N - n)}\\)</p> <p>Note that this functions assumes that the first observation is the oldest observation and the last observation is the newest observation. This is in line with the standard <code>pandas</code> way of sorting <code>pd.DataFrame</code>s with <code>Datetime</code>-indices.</p> <p>Numba</p> <p>This function uses <code>numba</code> just-in-time-compilation.</p> <p>Parameters:</p> <ul> <li> <code>forget</code>               (<code>float</code>)           \u2013            <p>Forget factor.</p> </li> <li> <code>size</code>               (<code>int</code>)           \u2013            <p>Length of the output vector.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Vector of exponentially discounted weights.</p> </li> </ul>"},{"location":"gram/#ondil.gram.init_gram","title":"ondil.gram.init_gram","text":"<pre><code>init_gram(\n    X: ndarray, w: ndarray, forget: float = 0\n) -&gt; np.ndarray\n</code></pre> <p>Initialise the Gramian Matrix.</p> <p>The Gramian Matrix is defined as $$ G = X^T \\Gamma WX $$ where \\(X\\) is the design matrix, \\(W\\) is a diagonal, user-defined weight matrix, \\(\\Gamma\\) is a diagonal matrix of exponentially discounting weights.</p> <p>Numba</p> <p>This function uses <code>numba</code> just-in-time-compilation.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Design matrix \\(X\\)</p> </li> <li> <code>w</code>               (<code>ndarray</code>)           \u2013            <p>Weights vector</p> </li> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Forget factor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Gramian Matrix.</p> </li> </ul>"},{"location":"gram/#ondil.gram.init_y_gram","title":"ondil.gram.init_y_gram","text":"<pre><code>init_y_gram(\n    X: ndarray, y: ndarray, w: ndarray, forget: float = 0\n) -&gt; np.ndarray\n</code></pre> <p>Initialise the y-Gramian Matrix.</p> <p>The Gramian Matrix is defined as $$ H = X^T \\Gamma WY $$ where \\(X\\) is the design matrix, \\(Y\\) is the response variable, \\(W\\) is a diagonal, user-defined weight matrix, \\(\\Gamma\\) is a diagonal matrix of exponentially discounting weights.</p> <p>Numba</p> <p>This function uses <code>numba</code> just-in-time-compilation.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Design matrix \\(X\\)</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>Response variable \\(Y\\)</p> </li> <li> <code>w</code>               (<code>ndarray</code>)           \u2013            <p>Weights vector</p> </li> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Forget factor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: y-Gramian Matrix.</p> </li> </ul>"},{"location":"gram/#ondil.gram.init_inverted_gram","title":"ondil.gram.init_inverted_gram","text":"<pre><code>init_inverted_gram(\n    X: ndarray, w: ndarray, forget: float = 0\n) -&gt; np.ndarray\n</code></pre> <p>Initialise the inverted Gramian Matrix.</p> <p>The inverted Gramian Matrix is defined as $$ G = (X^T \\Gamma WX)^{-1} $$ where \\(X\\) is the design matrix, \\(W\\) is a diagonal, user-defined weight matrix, \\(\\Gamma\\) is a diagonal matrix of exponentially discounting weights.</p> <p>Numba</p> <p>This function uses <code>numba</code> just-in-time-compilation.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Design matrix \\(X\\)</p> </li> <li> <code>w</code>               (<code>ndarray</code>)           \u2013            <p>Weights vector</p> </li> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Forget factor. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Gramian Matrix.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the matrix is not invertible (if rank(X.T @ X) &lt; X.shape[0]).</p> </li> </ul>"},{"location":"gram/#ondil.gram.update_gram","title":"ondil.gram.update_gram","text":"<pre><code>update_gram(\n    gram: ndarray,\n    X: ndarray,\n    forget: float = 0,\n    w: float = 1,\n) -&gt; np.ndarray\n</code></pre> <p>Update the Gramian Matrix.</p> <p>Numba</p> <p>This function uses <code>numba</code> just-in-time-compilation.</p> <p>Parameters:</p> <ul> <li> <code>gram</code>               (<code>ndarray</code>)           \u2013            <p>Gramian Matrix</p> </li> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>New observations for \\(X\\)</p> </li> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Forget factor. Defaults to 0.</p> </li> <li> <code>w</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Weights for the new observations. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Updated Gramian Matrix.</p> </li> </ul>"},{"location":"gram/#ondil.gram.update_y_gram","title":"ondil.gram.update_y_gram","text":"<pre><code>update_y_gram(\n    gram: ndarray,\n    X: ndarray,\n    y: ndarray,\n    forget: float = 0,\n    w: float = 1,\n) -&gt; np.ndarray\n</code></pre> <p>Update the Y-Gramian Matrix.</p> <p>Numba</p> <p>This function uses <code>numba</code> just-in-time-compilation.</p> <p>Parameters:</p> <ul> <li> <code>gram</code>               (<code>ndarray</code>)           \u2013            <p>Gramian Matrix</p> </li> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>New Observations for \\(X\\)</p> </li> <li> <code>y</code>               (<code>ndarray</code>)           \u2013            <p>New Observations for \\(Y\\)</p> </li> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Forget Factor. Defaults to 0.</p> </li> <li> <code>w</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Weights for the new observations. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Updated Gramian Matrix.</p> </li> </ul>"},{"location":"gram/#ondil.gram.update_inverted_gram","title":"ondil.gram.update_inverted_gram","text":"<pre><code>update_inverted_gram(\n    gram: ndarray,\n    X: ndarray,\n    forget: float = 0,\n    w: float = 1,\n) -&gt; np.ndarray\n</code></pre> <p>Update the inverted Gramian Matrix.</p> <p>Numba</p> <p>This function uses <code>numba</code> just-in-time-compilation.</p> <p>Parameters:</p> <ul> <li> <code>gram</code>               (<code>ndarray</code>)           \u2013            <p>Inverted Gramian Matrix.</p> </li> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>New observations for \\(X\\).</p> </li> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0</code> )           \u2013            <p>Forget Factor. Defaults to 0.</p> </li> <li> <code>w</code>               (<code>float</code>, default:                   <code>1</code> )           \u2013            <p>Weights for new observations. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Updated inverted Gramian matrix.</p> </li> </ul>"},{"location":"links/","title":"Link functions","text":""},{"location":"links/#link-functions_1","title":"Link functions","text":"<p>A link function \\(g(x)\\) is a smooth, monotonic function of \\(x\\). </p> <p>For all link functions, we implement </p> <ul> <li>the link \\(g(x)\\)</li> <li>the inverse \\(g^{-1}(x)\\)</li> <li>the derivative of the link function \\(\\frac{\\partial g(x)}{\\partial x}\\).</li> <li>the first derivative of the inverse of the link function \\(\\frac{\\partial g(x)^{-1}}{\\partial x}\\). The choice of the inverse is justified by Equation (7) in Hirsch, Berrisch &amp; Ziel (2024). </li> </ul> <p>The link functions implemented in <code>ondil</code> implemenent these as class methods each. Currently, we have implemented the identity-link, log-link and  shifted log-link functions.</p>"},{"location":"links/#overview-of-link-functions","title":"Overview of Link Functions","text":"Link Function Description <code>Identity</code> Implements the identity link function \\(g(x) = x\\). <code>Log</code> Implements the logarithmic link function \\(g(x) = \\log(x)\\). <code>LogShiftValue</code> Log link function with a shift value added to the inverse transformation. <code>LogShiftTwo</code> Log link function ensuring \\(\\hat{\\theta} &gt; 2\\). <code>LogIdent</code> Combines identity and log transformations. <code>Logit</code> Implements the logit link function \\(g(x) = \\log(x/(1-x))\\). <code>Sqrt</code> Implements the square root link function \\(g(x) = \\sqrt{x}\\). <code>SqrtShiftValue</code> Square root link function with a shift value added to the inverse transformation. <code>SqrtShiftTwo</code> Square root link function ensuring \\(\\hat{\\theta} &gt; 2\\). <code>InverseSoftPlus</code> Implements the inverse softplus link function. <code>InverseSoftPlusShiftValue</code> Inverse softplus link function with a shift value added to the inverse transformation. <code>InverseSoftPlusShiftTwo</code> Inverse softplus link function ensuring \\(\\hat{\\theta} &gt; 2\\)."},{"location":"links/#shifted-link-functions","title":"Shifted Link Functions","text":"<p>Some link functions implement shifted versions. The shifted link function is implemented in the sense that the shift is added to the inverse transformation. This way, we can ensure that distribution parameters can be modelled on the continuous space of the \\(\\eta = g(\\theta)\\), but in the inverse transform fullfill certain additional constraints. A common example is the \\(t\\) distribution, where we can use a <code>LogShift2</code> or <code>SqrtShift2Link</code> to ensure that \\(\\hat{\\theta} = g^-1(\\eta) &gt; 2\\) and the variance exists.</p>"},{"location":"links/#base-class","title":"Base Class","text":""},{"location":"links/#ondil.base.LinkFunction","title":"ondil.base.LinkFunction","text":"<p>               Bases: <code>ABC</code></p> <p>The base class for the link functions.</p>"},{"location":"links/#ondil.base.LinkFunction.link_support","title":"link_support  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>link_support: Tuple[float, float]\n</code></pre> <p>The support of the distribution.</p>"},{"location":"links/#ondil.base.LinkFunction.link","title":"link  <code>abstractmethod</code>","text":"<pre><code>link(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Calculate the Link</p>"},{"location":"links/#ondil.base.LinkFunction.inverse","title":"inverse  <code>abstractmethod</code>","text":"<pre><code>inverse(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Calculate the inverse of the link function</p>"},{"location":"links/#ondil.base.LinkFunction.link_derivative","title":"link_derivative  <code>abstractmethod</code>","text":"<pre><code>link_derivative(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Calculate the first derivative of the link function</p>"},{"location":"links/#ondil.base.LinkFunction.link_second_derivative","title":"link_second_derivative  <code>abstractmethod</code>","text":"<pre><code>link_second_derivative(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Calculate the second derivative for the link function</p>"},{"location":"links/#ondil.base.LinkFunction.inverse_derivative","title":"inverse_derivative  <code>abstractmethod</code>","text":"<pre><code>inverse_derivative(x: ndarray) -&gt; np.ndarray\n</code></pre> <p>Calculate the first derivative for the inverse link function</p>"},{"location":"links/#api-reference","title":"API Reference","text":""},{"location":"links/#ondil.links.Identity","title":"ondil.links.Identity","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The identity link function.</p> <p>The identity link is defined as \\(g(x) = x\\).</p>"},{"location":"links/#log-link-functions","title":"Log-Link Functions","text":""},{"location":"links/#ondil.links.Log","title":"ondil.links.Log","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The log-link function.</p> <p>The log-link function is defined as \\(g(x) = \\log(x)\\).</p>"},{"location":"links/#ondil.links.LogShiftValue","title":"ondil.links.LogShiftValue","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The Log-Link function shifted to a value \\(v\\).</p> <p>This link function is defined as \\(g(x) = \\log(x - v)\\). It can be used to ensure that certain distribution paramters don't fall below lower bounds, e.g. ensuring that the degrees of freedom of a Student's t distribtuion don't fall below 2, hence ensuring that the variance exists.</p>"},{"location":"links/#ondil.links.LogShiftTwo","title":"ondil.links.LogShiftTwo","text":"<p>               Bases: <code>LogShiftValue</code></p> <p>The Log-Link function shifted to 2.</p> <p>This link function is defined as \\(g(x) = \\log(x - 2)\\). It can be used to ensure that certain distribution paramters don't fall below lower bounds, e.g. ensuring that the degrees of freedom of a Student's t distribtuion don't fall below 2, hence ensuring that the variance exists.</p>"},{"location":"links/#ondil.links.LogIdent","title":"ondil.links.LogIdent","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The Logident Link function.</p> <p>The LogIdent Link function has been introduced by Narajewski &amp; Ziel 2020 and can be used to avoid the exponential inverse for large values while keeping the log-behaviour in small ranges. This can stabilize the estimation procedure.</p>"},{"location":"links/#logit-link-functions","title":"Logit Link Functions","text":""},{"location":"links/#ondil.links.Logit","title":"ondil.links.Logit","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The Logit Link function.</p> <p>The logit-link function is defined as \\(g(x) = \\log (x/ (1-x))\\).</p>"},{"location":"links/#square-root-link-functions","title":"Square Root Link Functions","text":""},{"location":"links/#ondil.links.Sqrt","title":"ondil.links.Sqrt","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The square root Link function.</p> <p>The square root link function is defined as \\(\\(g(x) = \\sqrt(x)\\)\\).</p>"},{"location":"links/#ondil.links.SqrtShiftValue","title":"ondil.links.SqrtShiftValue","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The Sqrt-Link function shifted to a value \\(v\\).</p> <p>This link function is defined as \\(\\(g(x) = \\sqrt(x - v)\\)\\). It can be used to ensure that certain distribution paramters don't fall below lower bounds, e.g. ensuring that the degrees of freedom of a Student's t distribtuion don't fall below 2, hence ensuring that the variance exists.</p>"},{"location":"links/#ondil.links.SqrtShiftTwo","title":"ondil.links.SqrtShiftTwo","text":"<p>               Bases: <code>SqrtShiftValue</code></p> <p>The Sqrt-Link function shifted to 2.</p> <p>This link function is defined as \\(\\(g(x) = \\sqrt(x - 2)\\)\\). It can be used to ensure that certain distribution paramters don't fall below lower bounds, e.g. ensuring that the degrees of freedom of a Student's t distribtuion don't fall below 2, hence ensuring that the variance exists.</p>"},{"location":"links/#inverse-softplus-link-functions","title":"Inverse SoftPlus Link Functions","text":""},{"location":"links/#ondil.links.InverseSoftPlus","title":"ondil.links.InverseSoftPlus","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The softplus is defined as $$     \\operatorname{SoftPlus(x)} = \\log(1 + \\exp(x)) $$ and hence the inverse is defined as $$     \\log(\\exp(x) - 1) $$ which can be used as link function for the parameters on the positive real line. The behavior of the inverse softplus is more graceful on large values as it avoids the exp of the log-link and converges towards a linear behaviour.</p> <p>The softplus is the smooth approximation of \\(\\max(x, 0)\\).</p>"},{"location":"links/#ondil.links.InverseSoftPlusShiftValue","title":"ondil.links.InverseSoftPlusShiftValue","text":"<p>               Bases: <code>LinkFunction</code></p> <p>The Inverse SoftPlus function shifted to a value \\(v\\).</p>"},{"location":"links/#ondil.links.InverseSoftPlusShiftTwo","title":"ondil.links.InverseSoftPlusShiftTwo","text":"<p>               Bases: <code>InverseSoftPlusShiftValue</code></p> <p>The Inverse SoftPlus function shifted to 2.</p>"},{"location":"methods/","title":"Estimation Methods","text":""},{"location":"methods/#overview","title":"Overview","text":"<p><code>EstimationMethod()</code> classes do the actual hard lifting of fitting coefficients (or weights). They take more technical parameters like the length of the regularization path or upper bounds on certain coefficients. These parameters depend on the individual estimation method. In general, we aim to provide sensible out-of-the-box defaults. This page explains the difference in detail. <code>Estimator</code> classes often take a method parameter, to which either a string or an instance of the <code>EstimationMethod()</code> can be passed, e.g.</p> <pre><code>from ondil.estimators import OnlineLinearModel\nfrom ondil.methods import LassoPath\n\nfit_intercept = True\nscale_inputs = True\n\nmodel = OnlineLinearModel(\n    method=\"lasso\",  # default parameters\n    fit_intercept=fit_intercept,\n    scale_inputs=scale_inputs,\n)\n# or equivalent\nmodel = OnlineLinearModel(\n    method=LassoPath(),  # default parameters\n    fit_intercept=fit_intercept,\n    scale_inputs=scale_inputs,\n)\n# or with user-defined parameters\nmodel = OnlineLinearModel(\n    method=LassoPath(\n        lambda_n=10\n    ),  # only 10 different regularization strengths\n    fit_intercept=fit_intercept,\n    scale_inputs=scale_inputs,\n)\n</code></pre> <p>More information on coordinate descent can also be found on this page and in the API Reference below.</p>"},{"location":"methods/#api-reference","title":"API Reference","text":"<p>Note</p> <p>We don't document the classmethods of the <code>EstimationMethod</code> since these are only used internally.</p>"},{"location":"methods/#ondil.methods.OrdinaryLeastSquares","title":"ondil.methods.OrdinaryLeastSquares","text":"<p>               Bases: <code>EstimationMethod</code></p> <p>Simple ordinary least squares respectively recursive least squares. No fancy parameters possible.</p>"},{"location":"methods/#ondil.methods.LassoPath","title":"ondil.methods.LassoPath","text":"<p>               Bases: <code>ElasticNetPath</code></p> <p>Path-based lasso estimation.</p> <p>The lasso method runs coordinate descent along a (geometric) decreasing grid of regularization strengths (lambdas). We automatically calculate the maximum regularization strength for which all (not-regularized) coefficients are 0. The lower end of the lambda grid is defined as \\(\\(\\lambda_\\min = \\lambda_\\max * \\varepsilon_\\lambda.\\)\\)</p> <p>We allow to pass user-defined lower and upper bounds for the coefficients. The coefficient bounds must be an <code>numpy</code> array of the length of <code>X</code> respectively of the number of variables in the equation plus the intercept, if you fit one. This allows to box-constrain the coefficients to a certain range.</p> <p>Furthermore, we allow to choose the start value, i.e. whether you want an update to be warm-started on the previous fit's path or on the previous reguarlization strength or an average of both. If your data generating process is rather stable, the <code>\"previous_fit\"</code> should give considerable speed gains, since warm starting on the previous strength is effectively batch-fitting.</p> <p>Lastly, we have some rather technical parameters like the number of coordinate descent iterations, whether you want to cycle randomly and for which tolerance you want to break. We use active set iterations, i.e. after the first coordinate-wise update for each regularization strength, only non-zero coefficients are updated.</p> <p>We use <code>numba</code> to speed up the coordinate descent algorithm.</p>"},{"location":"methods/#ondil.methods.LassoPath.__init__","title":"__init__","text":"<pre><code>__init__(\n    lambda_n: int = 100,\n    lambda_eps: float = 0.0001,\n    early_stop: int = 0,\n    start_value_initial: Literal[\n        \"previous_lambda\", \"previous_fit\", \"average\"\n    ] = \"previous_lambda\",\n    start_value_update: Literal[\n        \"previous_lambda\", \"previous_fit\", \"average\"\n    ] = \"previous_fit\",\n    selection: Literal[\"cyclic\", \"random\"] = \"cyclic\",\n    beta_lower_bound: ndarray | None = None,\n    beta_upper_bound: ndarray | None = None,\n    tolerance: float = 0.0001,\n    max_iterations: int = 1000,\n)\n</code></pre> <p>Initializes the lasso method with the specified parameters.</p> <p>Parameters:</p> <ul> <li> <code>lambda_n</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of lambda values to use in the path. Default is 100.</p> </li> <li> <code>lambda_eps</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Minimum lambda value as a fraction of the maximum lambda. Default is 1e-4.</p> </li> <li> <code>early_stop</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Early stopping criterion. Will stop if the number of non-zero parameters is reached. Default is 0 (no early stopping).</p> </li> <li> <code>start_value_initial</code>               (<code>Literal['previous_lambda', 'previous_fit', 'average']</code>, default:                   <code>'previous_lambda'</code> )           \u2013            <p>Method to initialize the start value for the first lambda. Default is \"previous_lambda\".</p> </li> <li> <code>start_value_update</code>               (<code>Literal['previous_lambda', 'previous_fit', 'average']</code>, default:                   <code>'previous_fit'</code> )           \u2013            <p>Method to update the start value for subsequent lambdas. Default is \"previous_fit\".</p> </li> <li> <code>selection</code>               (<code>Literal['cyclic', 'random']</code>, default:                   <code>'cyclic'</code> )           \u2013            <p>Method to select features during the path. Default is \"cyclic\".</p> </li> <li> <code>beta_lower_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Lower bound for the coefficients. Default is None.</p> </li> <li> <code>beta_upper_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Upper bound for the coefficients. Default is None.</p> </li> <li> <code>tolerance</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for the optimization. Default is 1e-4.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum number of iterations for the optimization. Default is 1000.</p> </li> </ul>"},{"location":"methods/#ondil.methods.Ridge","title":"ondil.methods.Ridge","text":"<p>               Bases: <code>EstimationMethod</code></p> <p>Single-lambda Ridge Estimation.</p> <p>The ridge method runs coordinate descent for a single lambda.</p> <p>We allow to pass user-defined lower and upper bounds for the coefficients. The coefficient bounds must be an <code>numpy</code> array of the length of <code>X</code> respectively of the number of variables in the equation plus the intercept, if you fit one. This allows to box-constrain the coefficients to a certain range.</p> <p>Lastly, we have some rather technical parameters like the number of coordinate descent iterations, whether you want to cycle randomly and for which tolerance you want to break. We use active set iterations, i.e. after the first coordinate-wise update for each regularization strength, only non-zero coefficients are updated.</p> <p>We use <code>numba</code> to speed up the coordinate descent algorithm.</p>"},{"location":"methods/#ondil.methods.Ridge.__init__","title":"__init__","text":"<pre><code>__init__(\n    lambda_reg: float | None = None,\n    start_beta: ndarray | None = None,\n    selection: Literal[\"cyclic\", \"random\"] = \"cyclic\",\n    beta_lower_bound: ndarray | None = None,\n    beta_upper_bound: ndarray | None = None,\n    tolerance: float = 0.0001,\n    max_iterations: int = 1000,\n)\n</code></pre> <p>Initializes the Ridge method with the specified parameters.</p> <p>Parameters:</p> <ul> <li> <code>lambda_reg</code>               (<code>float</code>, default:                   <code>None</code> )           \u2013            <p>Regularization parameter. Must be greater than 0. Higher values lead to more regularization. If not set, the average variance of the features is used as the default.</p> </li> <li> <code>selection</code>               (<code>Literal['cyclic', 'random']</code>, default:                   <code>'cyclic'</code> )           \u2013            <p>Method to select features during the path. Default is \"cyclic\".</p> </li> <li> <code>beta_lower_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Lower bound for the coefficients. Default is None.</p> </li> <li> <code>beta_upper_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Upper bound for the coefficients. Default is None.</p> </li> <li> <code>tolerance</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for the optimization. Default is 1e-4.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum number of iterations for the optimization. Default is 1000.</p> </li> </ul>"},{"location":"methods/#ondil.methods.ElasticNetPath","title":"ondil.methods.ElasticNetPath","text":"<p>               Bases: <code>EstimationMethod</code></p> <p>Path-based elastic net estimation.</p> <p>The elastic net method runs coordinate descent along a (geometric) decreasing grid of regularization strengths (lambdas). We automatically calculate the maximum regularization strength for which all (not-regularized) coefficients are 0. The lower end of the lambda grid is defined as \\(\\(\\lambda_\\min = \\lambda_\\max * \\varepsilon_\\lambda.\\)\\)</p> <p>The elastic net method is a combination of LASSO and Ridge regression. Parameter \\(\u0007lpha\\) controls the balance between LASSO and Ridge. Thereby, \\(\u0007lpha=0\\) corresponds to Ridge regression and \\(\u0007lpha=1\\) corresponds to LASSO regression.</p> <p>We allow to pass user-defined lower and upper bounds for the coefficients. The coefficient bounds must be an <code>numpy</code> array of the length of <code>X</code> respectively of the number of variables in the equation plus the intercept, if you fit one. This allows to box-constrain the coefficients to a certain range.</p> <p>Furthermore, we allow to choose the start value, i.e. whether you want an update to be warm-started on the previous fit's path or on the previous reguarlization strength or an average of both. If your data generating process is rather stable, the <code>\"previous_fit\"</code> should give considerable speed gains, since warm starting on the previous strength is effectively batch-fitting.</p> <p>Lastly, we have some rather technical parameters like the number of coordinate descent iterations, whether you want to cycle randomly and for which tolerance you want to break. We use active set iterations, i.e. after the first coordinate-wise update for each regularization strength, only non-zero coefficients are updated.</p> <p>We use <code>numba</code> to speed up the coordinate descent algorithm.</p>"},{"location":"methods/#ondil.methods.ElasticNetPath.__init__","title":"__init__","text":"<pre><code>__init__(\n    alpha: float,\n    lambda_n: int = 100,\n    lambda_eps: float = 0.0001,\n    early_stop: int = 0,\n    start_value_initial: Literal[\n        \"previous_lambda\", \"previous_fit\", \"average\"\n    ] = \"previous_lambda\",\n    start_value_update: Literal[\n        \"previous_lambda\", \"previous_fit\", \"average\"\n    ] = \"previous_fit\",\n    selection: Literal[\"cyclic\", \"random\"] = \"cyclic\",\n    beta_lower_bound: ndarray | None = None,\n    beta_upper_bound: ndarray | None = None,\n    tolerance: float = 0.0001,\n    max_iterations: int = 1000,\n)\n</code></pre> <p>Initializes the ElasticNet method with the specified parameters.</p> <p>Parameters:</p> <ul> <li> <code>alpha</code>               (<code>float</code>)           \u2013            <p>Mixing parameter between the L1 and L2 loss. Alpha = 0 corresponds to the Rigde, Alpha = 1 corresponds to the LASSO.</p> </li> <li> <code>lambda_n</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <p>Number of lambda values to use in the path. Default is 100.</p> </li> <li> <code>lambda_eps</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Minimum lambda value as a fraction of the maximum lambda. Default is 1e-4.</p> </li> <li> <code>early_stop</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Early stopping criterion. Will stop if the number of non-zero parameters is reached. Default is 0 (no early stopping).</p> </li> <li> <code>start_value_initial</code>               (<code>Literal['previous_lambda', 'previous_fit', 'average']</code>, default:                   <code>'previous_lambda'</code> )           \u2013            <p>Method to initialize the start value for the first lambda. Default is \"previous_lambda\".</p> </li> <li> <code>start_value_update</code>               (<code>Literal['previous_lambda', 'previous_fit', 'average']</code>, default:                   <code>'previous_fit'</code> )           \u2013            <p>Method to update the start value for subsequent lambdas. Default is \"previous_fit\".</p> </li> <li> <code>selection</code>               (<code>Literal['cyclic', 'random']</code>, default:                   <code>'cyclic'</code> )           \u2013            <p>Method to select features during the path. Default is \"cyclic\".</p> </li> <li> <code>beta_lower_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Lower bound for the coefficients. Default is None.</p> </li> <li> <code>beta_upper_bound</code>               (<code>ndarray | None</code>, default:                   <code>None</code> )           \u2013            <p>Upper bound for the coefficients. Default is None.</p> </li> <li> <code>tolerance</code>               (<code>float</code>, default:                   <code>0.0001</code> )           \u2013            <p>Tolerance for the optimization. Default is 1e-4.</p> </li> <li> <code>max_iterations</code>               (<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum number of iterations for the optimization. Default is 1000.</p> </li> </ul>"},{"location":"model_selection/","title":"Model Selection","text":"<p><code>ondil</code> employs online model selection based on information criteria (IC). We calculate the IC based on the Residual Sum of Squares (RSS), which can be tracked online.</p>"},{"location":"model_selection/#api-reference","title":"API Reference","text":""},{"location":"model_selection/#ondil.information_criteria.InformationCriterion","title":"ondil.information_criteria.InformationCriterion","text":"<p>Calculate the information criteria.</p> <p>+---------+--------------------------------------+--------------------------+ | <code>ic</code>    | Information Criterion                | Formula                  | +=========+======================================+==========================+ | <code>\"aic\"</code> | Akaike's Information Criterion       | \\(- 2l + 2p\\)              | | <code>\"aicc\"</code>| Corr. Akaike's Information Criterion | \\(- 2l + 2pn/(n-p-1)\\)     | | <code>\"bic\"</code> | Bayesian Information Criterion       | \\(- 2l + p\\log(n)\\)        | | <code>\"hqc\"</code> | Hannan-Quinn Information Criterion   | \\(- 2l + 2p\\log(\\log(n))\\) | | <code>\"max\"</code> | Select the largest model             |                          | +---------+--------------------------------------+--------------------------+</p>"},{"location":"model_selection/#ondil.information_criteria.InformationCriterion--methods","title":"Methods:","text":"<p>from_rss(rss)     Compute the chosen criterion from residual sum of squares. from_ll(log_likelihood)     Compute the chosen criterion directly from a log-likelihood value.</p>"},{"location":"model_selection/#ondil.information_criteria.InformationCriterion.__init__","title":"__init__","text":"<pre><code>__init__(\n    n_observations: Union[int, ndarray],\n    n_parameters: Union[int, ndarray],\n    criterion: Literal[\n        \"aic\", \"bic\", \"hqc\", \"aicc\", \"max\"\n    ] = \"aic\",\n)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>n_observations</code>               (<code>int or array - like</code>)           \u2013            <p>Number of observations used in the model.</p> </li> <li> <code>n_parameters</code>               (<code>int or array - like</code>)           \u2013            <p>Number of estimated parameters in the model.</p> </li> <li> <code>criterion</code>               (<code>{\"aic\",\"bic\",\"hqc\",\"aicc\", \"max\"}, default=\"aic\"</code>, default:                   <code>'aic'</code> )           \u2013            <p>The information criterion to compute.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the criterion is not recognized.</p> </li> </ul>"},{"location":"model_selection/#ondil.information_criteria.InformationCriterion.from_rss","title":"from_rss","text":"<pre><code>from_rss(\n    rss: Union[float, ndarray],\n) -&gt; Union[float, np.ndarray]\n</code></pre> <p>Compute the specified information criterion from the residual sum of squares (RSS).</p> The Gaussian log-likelihood is estimated as <p>ll = -n/2 * log(rss / n) - n/2 * (1 + log(2\u03c0))</p> <p>Parameters:</p> <ul> <li> <code>rss</code>               (<code>float or array - like</code>)           \u2013            <p>Residual sum of squares of the fitted model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ic</code> (              <code>float or array - like</code> )          \u2013            <p>The information criterion value (AIC, AICC, BIC, HQC, or Max).</p> </li> </ul>"},{"location":"model_selection/#ondil.information_criteria.InformationCriterion.from_ll","title":"from_ll","text":"<pre><code>from_ll(\n    log_likelihood: Union[float, ndarray],\n) -&gt; Union[float, np.ndarray]\n</code></pre> <p>Compute the specified information criterion directly from log-likelihood.</p> <p>Parameters:</p> <ul> <li> <code>log_likelihood</code>               (<code>float or array - like</code>)           \u2013            <p>The log-likelihood of the model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ic</code> (              <code>float or array - like</code> )          \u2013            <p>The information criterion value (AIC, AICC, BIC, HQC, or Max).</p> </li> </ul>"},{"location":"modeling_tips/","title":"Tips and Tricks","text":"<p>This page contains a collection of tips and tricks for using the online regression models.</p>"},{"location":"modeling_tips/#help-my-model-fails-to-converge","title":"Help! My model fails to converge","text":"<p>If your distributional regression model does not converge or fails to fit, you might want to check the following:</p> <ul> <li>Turn on the <code>verbose=3</code> option in the <code>Estimator()</code> class. This will print out the optimization steps and might give you a hint on what is going wrong.</li> <li>Turn on the <code>debug=True</code> option in the <code>Estimator()</code> class. That will save each iteration's data to the estimator class. Remember to remove the option for production settings, otherwise the model size can increase significantly.</li> <li>Check the data for missing values.</li> <li>Check the data for features with zero variance. These cannot be handled by the <code>OnlineScaler()</code> and will cause missing / infinite values (due to the division by zero).</li> <li>Is the distribution you're imposing on the data appropriate? Likelihood-based methods fail miserably if the distribution is not appropriate. This especially concerns heavy tails, skewness, and, potentially, distributions that only live on the positive side of the real line.</li> </ul> <p>If you have answered all of the above questions and the model still does not converge, please open an issue with a reproducible example. We will help you as best as we can.</p>"},{"location":"modeling_tips/#scaling","title":"Scaling","text":"<p>Keep in mind which variables you'd like to scale, especially if you use lagged instances of the target variable. As the target variable \\(y\\) is not scaled in the models, but all variables in \\(X\\) are scaled (by default), the kind of 1:1 relationship you usually expect with lagged instances gets impacted. In the worst case, if a severe distribution shift occurs in \\(y\\), the model might starkly drift in an online prediction setting since \\(L^l(y)\\) are scaled with increasing/decreasing variance of \\(y\\), but \\(y\\) itself is not. </p>"},{"location":"multivariate/","title":"Multivariate distributional regression","text":"<p>This example demonstrates how to perform multivariate distributional regression using the <code>ondil</code> package. We will generate synthetic data from a multivariate normal distribution and fit a model to estimate the parameters of the distribution. The example follows the simulation study of Muschinski, Thomas, et al. \"Cholesky-based multivariate Gaussian regression.\" Econometrics and Statistics 29 (2024): 261-281.</p>"},{"location":"multivariate/#introduction","title":"Introduction","text":"<p>Multivariate distributional regression allows us to model mean and covariance matrix of a multivariate response variable as functions of covariates. That is, we have a response variable \\(Y \\in \\mathbb{R}^D\\) and covariates \\(X \\in \\mathbb{R}^p\\), and we want to model the conditional distribution \\(Y|X\\) as a multivariate normal distribution with mean vector \\(\\mu(X)\\) and covariance matrix \\(\\Sigma(X)\\). We often have to parameterize the covariance matrix to ensure it is positive definite. A common approach is to use the Cholesky decomposition, where \\(\\Sigma(X) = L(X)L(X)^T\\) and \\(L(X)\\) is a lower triangular matrix with positive diagonal entries, alternatively the modified Cholesky decomposition can be used.</p>"},{"location":"multivariate/#model","title":"Model","text":"<p>We want to model $$ Y \\sim \\mathcal{N}(\\mu(x), \\Sigma(x)) $$ where \\(y\\) is a \\(D\\)-dimensional response vector, \\(x\\) is a \\(p\\)-dimensional covariate vector, \\(\\mu(x)\\) is the mean vector, and \\(\\Sigma(x)\\) is the covariance matrix. The parameters of the model are functions of the covariates, which can be modeled using linear or non-linear functions. We use the two versions of the Cholesky decomposition implemented in <code>ondil</code>: the standard Cholesky decomposition and the modified Cholesky decomposition. Both are implemented based on the precision matrix of the multivariate normal \\(\\Omega = \\Sigma^{-1}\\).</p> <ul> <li>Cholesky decomposition: \\(\\Omega = L^T L\\), where \\(L\\) is a lower triangular matrix with positive diagonal entries. The parameters to be estimated are the elements of \\(L\\).</li> <li>Modified Cholesky decomposition: \\(\\Omega = T^T D^{-1} T\\), where \\(D\\) is a diagonal matrix with positive entries and \\(T\\) is a lower triangular matrix with ones on the diagonal. The parameters to be estimated are the elements of the diagonal of \\(D\\) and the elements of \\(T\\) below the diagonal.</li> </ul> <p>The data set is generated by first sampling values of \\(x\\) from a uniform distribution over the interval \\([0, 1]\\). For each sampled \\(x\\), a 3-dimensional vector \\(y\\) is drawn from a trivariate Gaussian distribution, where the distribution's parameters are functions of \\(x\\). Each parameter incorporates a combination of constant, linear, and quadratic effects. Denote with \\(\\psi\\) the elements of the diagonal vector of the modified Cholesky decomposition of the precision matrix, and with \\(\\phi\\) the elements of the lower triangular matrix. The true functions for the mean vector \\(\\mu\\), the log of the diagonal elements of the precision matrix \\(\\log(\\psi)\\), and the lower triangular elements \\(\\phi\\) are given in the table below.</p> \\(\\mu\\) \\(\\psi\\) \\(\\phi\\) \\(\\mu_1(x) = 1\\) \\(\\log(\\psi_1(x)) = -2\\) \\(\\phi_{1,2}(x) = \\frac{1 + x^2}{4}\\) \\(\\mu_2(x) = 1 + x\\) \\(\\log(\\psi_2(x)) = -2 + x\\) \\(\\phi_{1,3}(x) = 0\\) \\(\\mu_3(x) = 1 + x^2\\) \\(\\log(\\psi_3(x)) = -2 + x^2\\) \\(\\phi_{2,3}(x) = \\frac{3 + x}{4}\\) <p>We then model each element of \\(\\mu(x)\\), \\(\\log(\\text{diag}(D))\\), and the elements of \\(T\\) below the diagonal as B-Spline of the explanatory variable \\(x\\).</p> <ul> <li>For the mean $ \\mu_i(x) = \\sum_{j=1}^{K} \\beta_{ij} B_j(x) \\quad \\text{for } i = 1, \\ldots, D $</li> <li>For the diagonal elements $ \\log(\\psi_i(x)) = \\sum_{j=1}^{K} \\gamma_{ij} B_j(x) \\quad \\text{for } i = 1, \\ldots, D $</li> <li>For the off-diagonal elements $ \\phi_{i,j}(x) = \\sum_{k=1}^{K} \\delta_{ijk} B_k(x) \\quad \\text{for } 1 \\leq j &lt; i \\leq D $</li> </ul> <p>The figures below show the true and estimated functions for the mean vector and the log of the diagonal elements of the precision matrix, as well as the true and estimated covariance matrices for a three-dimensional example with \\(D=3\\) and \\(M=10000\\) samples and for \\(D=10\\) and \\(M=10000\\) samples.</p>"},{"location":"multivariate/#implementation","title":"Implementation","text":"<p>We use the following code to implement the multivariate distributional regression model using the <code>ondil</code> package. The code for generating the figures is provided in the file <code>examples/multivariate.py</code> in the examples folder. The snippet below shows the main parts of the implementation for \\(D=3\\).</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as st\nfrom sklearn.preprocessing import SplineTransformer\n\nfrom ondil.distributions import (\n    MultivariateNormalInverseCholesky,\n    MultivariateNormalInverseModifiedCholesky,\n)\nfrom ondil.estimators import MultivariateOnlineDistributionalRegressionPath\n\nnp.set_printoptions(precision=3, suppress=True)\n\n# Dimension of the data D=3\n# Initial training size M=1000\n# On my laptop\n# For 10k samples and D=10, it takes a two-three mins.\n# For 10k samples and D=3, it takes a few seconds.\n\nD = 3\nM = 1000\n\n\n# Define a function to compute the true parameters\ndef compute_true(x, D=3):\n    M = len(x)\n    DD = (D // 3 + 1) * 3\n\n    true_mu = np.zeros((M, DD))\n    true_D = np.zeros((M, DD, DD))\n    true_T = np.zeros((M, DD, DD))\n\n    for i in range(0, D + 1, 3):\n        true_mu[:, i + 0] = 1\n        true_mu[:, i + 1] = 1 + x\n        true_mu[:, i + 2] = 1 + x**2\n\n        true_D[:, i + 0, i + 0] = np.exp(-2)\n        true_D[:, i + 1, i + 1] = np.exp(-2 + x)\n        true_D[:, i + 2, i + 2] = np.exp(-2 + x**2)\n\n        true_T[:, i + 0, i + 1] = (1 + x**2) / 4\n        true_T[:, i + 0, i + 2] = 0\n        true_T[:, i + 1, i + 2] = (3 + x) / 4\n\n    true_T[:, range(DD), range(DD)] = -1\n    true_T = true_T[:, :D, :D]\n\n    true_D = true_D[:, :D, :D]\n    true_mu = true_mu[:, :D]\n\n    return true_mu, true_D, true_T\n\n\ndistribution = MultivariateNormalInverseCholesky()\nequation = {\n    0: {d: \"all\" for d in range(D)},\n    1: {k: \"all\" for k in range((D * (D + 1)) // 2)},\n}\n\nr = 0\nx = st.uniform(-1, 2).rvs((M), random_state=1 + r)\ntrue_mu, true_D, true_T = compute_true(x, D=D)\ntrue_cov = np.linalg.inv(true_T @ np.linalg.inv(true_D) @ true_T.transpose(0, 2, 1))\n\ncorr = true_cov / (\n    true_cov.diagonal(axis1=1, axis2=2)[..., None] ** 0.5\n    @ true_cov.diagonal(axis1=1, axis2=2)[:, None, :] ** 0.5\n)\n\ny = np.zeros((M, D))\nfor m in range(M):\n    y[m, :] = st.multivariate_normal(true_mu[m], true_cov[m]).rvs(1)\n\ntransformer = SplineTransformer(\n    n_knots=4, degree=3, include_bias=False, extrapolation=\"linear\"\n)\ntransformer.fit(np.expand_dims(x, -1))\nX = transformer.transform(np.expand_dims(x, -1))\n\n# Model\nestimator_chol = MultivariateOnlineDistributionalRegressionPath(\n    distribution=distribution,\n    equation=equation,\n    method={i: \"ols\" for i in range(D)},\n    early_stopping=False,\n    early_stopping_criteria=\"bic\",\n    iteration_along_diagonal=False,\n    max_regularisation_size=None,\n    verbose=2,\n)\n\n# Fit the estimator\nestimator_chol.fit(X, y)\n</code></pre>"},{"location":"multivariate/#results","title":"Results","text":"<p>Below we show results for two different dimensions of the response variable \\(D=3\\) and \\(D=10\\). We use \\(M=10000\\) samples in both cases. The results are shown for both the Cholesky decomposition and the modified Cholesky decomposition.</p>"},{"location":"multivariate/#low-dimensional-example","title":"Low-dimensional example","text":"<p>We estimated the following covariance matrices depending on \\(x\\):</p> <p></p> <p>True (Black) and estimated Covariance matrix (Colored) for a three-dimensional example D=3 and M=10000 samples using the Cholesky decomposition.</p> <p></p> <p>True (Black) and estimated Covariance matrix (Colored) for a three-dimensional example D=3 and M=10000 samples using the modified Cholesky decomposition.</p>"},{"location":"multivariate/#high-dimensional-example","title":"High-dimensional example","text":"<p>For the higher-dimensional example, we estimated the following covariance matrices depending on \\(x\\). Here we see that the B-Splines get more wiggly, which is expected due to the higher dimensionality and the same number of samples. We can try to counter this behavior by employing some kind of regularization, e.g., ridge or lasso, since these are also implemented in <code>ondil</code>.</p> <p></p> <p>True (Black) and estimated Covariance matrix (Colored) for a three-dimensional example D=10 and M=10000 samples using the Cholesky decomposition.</p> <p></p> <p>True (Black) and estimated Covariance matrix (Colored) for a three-dimensional example D=10 and M=10000 samples using the modified Cholesky decomposition.</p>"},{"location":"scaler/","title":"Online Scaling and Preproessing","text":""},{"location":"scaler/#api-reference","title":"API Reference","text":""},{"location":"scaler/#ondil.scaler.OnlineScaler","title":"ondil.scaler.OnlineScaler","text":"<p>               Bases: <code>OndilEstimatorMixin</code>, <code>TransformerMixin</code>, <code>BaseEstimator</code></p>"},{"location":"scaler/#ondil.scaler.OnlineScaler.std_","title":"std_  <code>property</code>","text":"<pre><code>std_: float | ndarray\n</code></pre> <p>Standard deviation of the scaled variables.</p>"},{"location":"scaler/#ondil.scaler.OnlineScaler.__init__","title":"__init__","text":"<pre><code>__init__(\n    forget: float = 0.0, to_scale: bool | ndarray = True\n)\n</code></pre> <p>The online scaler allows for incremental updating and scaling of matrices.</p> <p>Parameters:</p> <ul> <li> <code>forget</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <p>The forget factor. Older observations will be exponentially discounted. Defaults to 0.0.</p> </li> <li> <code>to_scale</code>               (<code>bool | ndarray</code>, default:                   <code>True</code> )           \u2013            <p>The variables to scale. <code>True</code> implies all variables will be scaled. <code>False</code> implies no variables will be scaled. An <code>np.ndarray</code> of type <code>bool</code> or <code>int</code> implies that the columns <code>X[:, to_scale]</code> will be scaled, all other columns will not be scaled. Defaults to True.</p> </li> </ul>"},{"location":"scaler/#ondil.scaler.OnlineScaler.fit","title":"fit","text":"<pre><code>fit(\n    X: ndarray,\n    y: None = None,\n    sample_weight: ndarray | None = None,\n) -&gt; OnlineScaler\n</code></pre> <p>Fit the OnlineScaler() Object for the first time.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Matrix of covariates X.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Not used, present for compatibility with sklearn API. Defaults to None.</p> </li> <li> <code>sample_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Weights for each sample. Defaults to None (uniform weights).</p> </li> </ul>"},{"location":"scaler/#ondil.scaler.OnlineScaler.update","title":"update","text":"<pre><code>update(X: ndarray, y=None, sample_weight: ndarray = None)\n</code></pre> <p>Update the <code>OnlineScaler()</code> for new rows of X.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>New data for X.</p> </li> <li> <code>y</code>               (<code>None</code>, default:                   <code>None</code> )           \u2013            <p>Not used, present for compatibility with sklearn API. Defaults to None.</p> </li> <li> <code>sample_weight</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <p>Weights for each sample. Defaults to None (uniform weights).</p> </li> </ul>"},{"location":"scaler/#ondil.scaler.OnlineScaler.transform","title":"transform","text":"<pre><code>transform(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Transform X to a mean-std scaled matrix.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>X matrix for covariates.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Scaled X matrix.</p> </li> </ul>"},{"location":"scaler/#ondil.scaler.OnlineScaler.inverse_transform","title":"inverse_transform","text":"<pre><code>inverse_transform(X: ndarray) -&gt; np.ndarray\n</code></pre> <p>Back-transform a scaled X matrix to the original domain.</p> <p>Parameters:</p> <ul> <li> <code>X</code>               (<code>ndarray</code>)           \u2013            <p>Scaled X matrix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>           \u2013            <p>np.ndarray: Scaled back to the original scale.</p> </li> </ul>"}]}